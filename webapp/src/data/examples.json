{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading\n# We use the standardized penguins.csv dataset\ndf = pd.read_csv(DATA_PATH)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options (Selective columns and handling NaNs)\ndf = pd.read_csv(\n    DATA_PATH,\n    usecols=[\"species\", \"island\", \"bill_length_mm\"],\n    na_values=[\"NA\"],\n)\nprint(\"\\nSelective columns:\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded 344 rows:\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nSelective columns:\n  species     island  bill_length_mm\n0  Adelie  Torgersen            39.1\n1  Adelie  Torgersen            39.5\n2  Adelie  Torgersen            40.3\n3  Adelie  Torgersen             NaN\n4  Adelie  Torgersen            36.7\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading (eager)\n# Polars is extremely fast at CSV parsing\ndf = pl.read_csv(DATA_PATH)\nprint(\"Polars loaded penguins dataset:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(DATA_PATH)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded penguins dataset:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193               \u2506 3450        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading via SQL\n# DuckDB can query CSV files directly\nprint(\"DuckDB querying penguins via SQL:\")\nduckdb.sql(f\"SELECT * FROM '{DATA_PATH}' LIMIT 5\").show()",
    "language": "python",
    "output": "DuckDB querying penguins via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502               193 \u2502        3450 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.penguins\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()  # Wait for the job to complete\n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.penguins\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet\n# Pandas uses the pyarrow engine by default for Parquet\ndf = pd.read_parquet(DATA_PATH)\nprint(f\"Pandas loaded penguins Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded penguins Parquet with 344 rows.\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet (Eager)\n# Polars is built on Apache Arrow for ultra-fast Parquet performance\ndf = pl.read_parquet(DATA_PATH)\nprint(\"Polars loaded penguins Parquet:\")\nprint(df.head())\n\n# Scan Parquet (Lazy - Recommended for Large Data)\n# This allows Polars to skip reading columns/rows not needed\nquery = pl.scan_parquet(DATA_PATH).select([\"species\", \"island\"]).limit(5)\nprint(\"\\nLazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded penguins Parquet:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLazy scan result:\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2502\n\u2502 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to Parquet\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{DATA_PATH}' (FORMAT PARQUET)\")\n\n# %%\n# Query Parquet directly via SQL\n# DuckDB treats Parquet files as virtual tables\nprint(\"DuckDB querying penguins Parquet via SQL:\")\nduckdb.sql(f\"SELECT species, island FROM '{DATA_PATH}' LIMIT 5\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(DATA_PATH))\nprint(\"\\nRead as relation (First 5):\")\nrel.limit(5).show()",
    "language": "python",
    "output": "DuckDB querying penguins Parquet via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nRead as relation (First 5):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing (Local mock data)\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    import pandas as pd # Needed for conversion\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.penguins_parquet\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\n# Parquet is the recommended format for BigQuery due to schema persistence\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.penguins_parquet\n"
  },
  "load_json_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Pandas\n# Handling standard JSON and Newline Delimited JSON (JSONL).\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    df_temp = pd.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.to_json(JSON_PATH, orient=\"records\")\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Load standard JSON\ndf_json = pd.read_json(JSON_PATH)\nprint(\"Standard JSON (Pandas):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (lines=True)\ndf_ndjson = pd.read_json(NDJSON_PATH, lines=True)\nprint(\"\\nNDJSON (lines=True):\")\nprint(df_ndjson.head())",
    "language": "python",
    "output": "Standard JSON (Pandas):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nNDJSON (lines=True):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_json_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Polars\n# Fast parsing for structured and semi-structured data.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    \n    # Use Polars itself for high-performance conversion\n    df_temp = pl.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.write_json(JSON_PATH)\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.write_ndjson(NDJSON_PATH)\n\n# %%\n# Load standard JSON (Eager)\n# Polars parses standard JSON into memory efficiently\ndf_json = pl.read_json(JSON_PATH)\nprint(\"Standard JSON (Polars):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (Fastest)\n# Newline Delimited JSON is the preferred format for high-speed Polars loading\ndf_ndjson = pl.read_ndjson(NDJSON_PATH)\nprint(\"\\nNDJSON (Polars):\")\nprint(df_ndjson.head())\n\n# Scan NDJSON (Lazy - Great for large logs)\n# df_lazy = pl.scan_ndjson(ndjson_path).collect()",
    "language": "python",
    "output": "Standard JSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNDJSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_json_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with DuckDB\n# Direct SQL queries on JSON and NDJSON files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to JSON and NDJSON\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{JSON_PATH}' (FORMAT JSON, ARRAY TRUE)\")\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{NDJSON_PATH}' (FORMAT JSON)\")\n\n# %%\n# Query JSON directly via SQL\n# DuckDB treats JSON files as virtual tables with auto-schema detection\nprint(\"Standard JSON via SQL:\")\nduckdb.sql(f\"SELECT species, island, island FROM read_json_auto('{JSON_PATH}') LIMIT 5\").show()\n\n# Query NDJSON\nprint(\"\\nNDJSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{NDJSON_PATH}') LIMIT 5\").show()",
    "language": "python",
    "output": "Standard JSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nNDJSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_json_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with BigQuery\n# Loading NDJSON data into BigQuery tables. Note: BigQuery requires NDJSON (Newline Delimited) for loading local files.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert to NDJSON (Native to BigQuery load)\nif not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\ntable_id = \"project.dataset.penguins_json\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n    autodetect=True,\n)\n\n# %%\n# Load NDJSON to BigQuery\n# BigQuery requires Newline Delimited JSON (NDJSON) for direct file loads\nwith open(NDJSON_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery JSON load (NDJSON) triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery JSON load (NDJSON) triggered for project.dataset.penguins_json\n"
  },
  "load_database_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists (Self-healing)\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        # Create dummy data\n        df_dummy = pd.DataFrame({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n            \"dept\": [\"HR\", \"IT\", \"IT\", \"Finance\", \"HR\"],\n            \"salary\": [60000, 80000, 75000, 90000, 62000]\n        })\n        df_dummy.to_sql(\"employees\", conn, index=False)\n\n# 2. Main: Read from Database\n# Use a context manager to ensure connection closes\nwith sqlite3.connect(db_path) as conn:\n    # Read entire table\n    df = pd.read_sql(\"SELECT * FROM employees\", conn)\n    print(\"--- All Employees ---\")\n    print(df.head())\n    \n    # Read with filter query\n    query = \"SELECT name, salary FROM employees WHERE dept = 'IT'\"\n    df_it = pd.read_sql(query, conn)\n    print(\"\\n--- IT Department ---\")\n    print(df_it)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (requires psycopg2)\n# conn_str = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# MS SQL Server (requires pyodbc)\n# conn_str = \"mssql+pyodbc://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# IBM DB2 (requires ibm_db_sa)\n# conn_str = \"ibm_db_sa://user:password@host:port/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)",
    "language": "python",
    "output": "--- All Employees ---\n   id     name     dept  salary\n0   1    Alice       HR   60000\n1   2      Bob       IT   80000\n2   3  Charlie       IT   75000\n3   4    David  Finance   90000\n4   5      Eve       HR   62000\n\n--- IT Department ---\n      name  salary\n0      Bob   80000\n1  Charlie   75000\n"
  },
  "load_database_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"connectorx\",\n#     \"pyarrow\", \n# ]\n# ///\nimport polars as pl\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n        conn.execute(\"INSERT INTO employees VALUES (3, 'Charlie', 'IT', 75000)\")\n\n# 2. Main: Read from Database\n# Polars uses connectorx or adbc for high performance\nuri = f\"sqlite://{db_path}\"\n\nquery = \"SELECT * FROM employees\"\ndf = pl.read_database_uri(query=query, uri=uri)\n\nprint(\"--- Polars DataFrame ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (High Permance via ConnectorX)\n# uri = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# MS SQL Server (High Performance via ConnectorX)\n# uri = \"mssql://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# IBM DB2 (via SQLAlchemy fallback)\n# import sqlalchemy\n# engine = sqlalchemy.create_engine(\"ibm_db_sa://user:password@host:port/mydb\")\n# df = pl.read_database(\"SELECT * FROM employees\", connection=engine)",
    "language": "python",
    "output": "--- Polars DataFrame ---\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept    \u2506 salary \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2506 ---    \u2502\n\u2502 i64 \u2506 str     \u2506 str     \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 HR      \u2506 60000  \u2502\n\u2502 2   \u2506 Bob     \u2506 IT      \u2506 80000  \u2502\n\u2502 3   \u2506 Charlie \u2506 IT      \u2506 75000  \u2502\n\u2502 4   \u2506 David   \u2506 Finance \u2506 90000  \u2502\n\u2502 5   \u2506 Eve     \u2506 HR      \u2506 62000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_database_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\nimport duckdb\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n\n# 2. Main: Read from Database\n# DuckDB can query SQLite files directly without importing!\n# This is \"zero-copy\" - it reads the file format directly.\n\n# Install/Load sqlite extension (handled automatically by modern DuckDB, but good to know)\nduckdb.sql(\"INSTALL sqlite; LOAD sqlite;\")\n\n# Query directly\nquery = f\"SELECT * FROM sqlite_scan('{db_path}', 'employees')\"\ndf = duckdb.sql(query).show()\n\n# You can also attach it as a database\nprint(\"\\n--- Attached Database ---\")\nduckdb.sql(f\"ATTACH '{db_path}' AS mydb (TYPE SQLITE)\")\nduckdb.sql(\"SELECT name, salary FROM mydb.employees WHERE salary > 70000\").show()\n\n# ---------------------------------------------------------\n# PRO TIP: Enterprise Data Federation\n# ---------------------------------------------------------\n# PostgreSQL\n# duckdb.sql(\"INSTALL postgres; LOAD postgres;\")\n# duckdb.sql(\"ATTACH 'dbname=mydb user=user password=pass host=localhost' AS my_pg (TYPE POSTGRES)\")\n# duckdb.sql(\"SELECT * FROM my_pg.employees\").show()\n\n# MySQL / MariaDB\n# duckdb.sql(\"INSTALL mysql; LOAD mysql;\")\n# duckdb.sql(\"ATTACH 'user=user password=pass database=mydb' AS my_mysql (TYPE MYSQL)\")\n# duckdb.sql(\"SELECT * FROM my_mysql.employees\").show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502  dept   \u2502 salary \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502 HR      \u2502  60000 \u2502\n\u2502     2 \u2502 Bob     \u2502 IT      \u2502  80000 \u2502\n\u2502     3 \u2502 Charlie \u2502 IT      \u2502  75000 \u2502\n\u2502     4 \u2502 David   \u2502 Finance \u2502  90000 \u2502\n\u2502     5 \u2502 Eve     \u2502 HR      \u2502  62000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Attached Database ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502 salary \u2502\n\u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Bob     \u2502  80000 \u2502\n\u2502 Charlie \u2502  75000 \u2502\n\u2502 David   \u2502  90000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_database_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"db-dtypes\",\n#     \"pyarrow\", \n# ]\n# ///\nfrom google.cloud import bigquery\nimport os\n\n# Note: BigQuery cannot read your local SQLite file.\n# In production, you would use \"Federated Queries\" (EXTERNAL_QUERY) \n# to query a Cloud SQL (Postgres/MySQL) database without moving data.\n\n# Pseudo-code for simulation since we don't have active credentials\nprint(\"--- BigQuery Federated Query ---\")\n\nsql = \"\"\"\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\"\"\"\n\nprint(f\"Executing Query:\\n{sql}\")\n\n# In a real environment:\n# client = bigquery.Client()\n# df = client.query(sql).to_dataframe()\n# print(df)\n\nprint(\"\\n[Simulation] Result DataFrame:\")\nprint(\"   id     name dept  salary\")\nprint(\"0   2      Bob   IT   80000\")\nprint(\"1   3  Charlie   IT   75000\")",
    "language": "python",
    "output": "--- BigQuery Federated Query ---\nExecuting Query:\n\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\n\n[Simulation] Result DataFrame:\n   id     name dept  salary\n0   2      Bob   IT   80000\n1   3  Charlie   IT   75000\n"
  },
  "load_cloud_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"s3fs\",\n#     \"gcsfs\",\n#     \"pyarrow\",\n# ]\n# ///\nimport pandas as pd\nimport unittest.mock as mock\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n# Public dataset: NYC Taxi Data (Yellow Taxi, Jan 2023)\n# Note: Real S3 access requires AWS credentials configured.\n# We wrap this in a try/except block for demonstration resilience.\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to read from {S3_URI}...\")\n    # 'anon': True is required for public buckets without credentials\n    # Adding timeouts to fail fast if network is down\n    df_s3 = pd.read_parquet(S3_URI, storage_options={\n        \"anon\": True,\n        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n    })\n    print(\"Success natively reading S3!\")\n    print(df_s3.head())\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    # Mock DataFrame\n    df_s3 = pd.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n        \"passenger_count\": [1.0, 1.0, 2.0],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3.head())\n\n\n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# Public dataset: BigQuery Public Data (US States)\n\nGCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {GCS_URI}...\")\n    df_gcs = pd.read_csv(GCS_URI)\n    print(\"Success natively reading GCS!\")\n    print(df_gcs.head())\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pd.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs.head())",
    "language": "python",
    "output": "Attempting to read from s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (AioSession._create_client() got an unexpected keyword argument 'connect_timeout'). Mocking success for demo.\n   VendorID tpep_pickup_datetime  passenger_count  trip_distance  total_amount\n0         1  2023-01-01 00:32:10              1.0           0.97           9.3\n1         2  2023-01-01 00:55:08              1.0           1.10          14.3\n2         1  2023-01-01 00:25:04              2.0           0.20          12.3\n\nAttempting to read from gs://cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS!\n         name post_abbr\n0     Alabama        AL\n1      Alaska        AK\n2     Arizona        AZ\n3    Arkansas        AR\n4  California        CA\n"
  },
  "load_cloud_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"s3fs\",\n#     \"gcsfs\",\n#     \"fsspec\",\n# ]\n# ///\nimport polars as pl\nimport unittest.mock as mock\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n# Using NYC Taxi Data (Parquet) - Public Bucket\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to scan from {S3_URI}...\")\n    # Polars scan_parquet is lazy and highly optimized for cloud\n    # storage_options={\"anon\": True} for public buckets\n    # Adding timeouts to fail fast\n    q = pl.scan_parquet(S3_URI, storage_options={\n        \"anon\": True,\n        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n    })\n    \n    # Collect first 5 rows\n    df_s3 = q.limit(5).collect()\n    print(\"Success natively reading S3!\")\n    print(df_s3)\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    df_s3 = pl.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3)\n\n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# using BigQuery public data (CSV)\n\nGCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {GCS_URI}...\")\n    df_gcs = pl.read_csv(GCS_URI)\n    print(\"Success natively reading GCS!\")\n    print(df_gcs.head())\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pl.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs)",
    "language": "python",
    "output": "Attempting to scan from s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (invalid value for 'anon': 'True' (expected str)). Mocking success for demo.\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 VendorID \u2506 tpep_pickup_datetime \u2506 trip_distance \u2506 total_amount \u2502\n\u2502 ---      \u2506 ---                  \u2506 ---           \u2506 ---          \u2502\n\u2502 i64      \u2506 str                  \u2506 f64           \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 2023-01-01 00:32:10  \u2506 0.97          \u2506 9.3          \u2502\n\u2502 2        \u2506 2023-01-01 00:55:08  \u2506 1.1           \u2506 14.3         \u2502\n\u2502 1        \u2506 2023-01-01 00:25:04  \u2506 0.2           \u2506 12.3         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAttempting to read from gs://cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS!\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name       \u2506 post_abbr \u2502\n\u2502 ---        \u2506 ---       \u2502\n\u2502 str        \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alabama    \u2506 AL        \u2502\n\u2502 Alaska     \u2506 AK        \u2502\n\u2502 Arizona    \u2506 AZ        \u2502\n\u2502 Arkansas   \u2506 AR        \u2502\n\u2502 California \u2506 CA        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_cloud_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to query {S3_URI}...\")\n    # DuckDB's httpfs extension handles S3/GCS\n    duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n    \n    # Configure for anonymous access usually works for public buckets\n    # duckdb.sql(\"SET s3_region='us-east-1';\")\n    \n    # Query directly\n    df_s3 = duckdb.sql(f\"SELECT * FROM read_parquet('{S3_URI}') LIMIT 5\").df()\n    print(\"Success natively reading S3!\")\n    print(df_s3)\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    # Mock output\n    df_s3 = pd.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3)\n    \n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# DuckDB reads GCS via S3 compatibility layer or HTTPS for public files.\n# For public files, HTTPS is robust.\nHTTPS_URI = \"https://storage.googleapis.com/cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {HTTPS_URI}...\")\n    df_gcs = duckdb.sql(f\"SELECT * FROM read_csv_auto('{HTTPS_URI}') LIMIT 5\").df()\n    print(\"Success natively reading GCS (via HTTPS)!\")\n    print(df_gcs)\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pd.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs)",
    "language": "python",
    "output": "Attempting to query s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (HTTP Error: HTTP GET error on 'https://nyc-tlc.s3.amazonaws.com/trip%20data/yellow_tripdata_2023-01.parquet' (HTTP 403)\n\nAuthentication Failure - this is usually caused by invalid or missing credentials.\n* No credentials are provided.\n* See https://duckdb.org/docs/stable/extensions/httpfs/s3api.html). Mocking success for demo.\n   VendorID  trip_distance  total_amount\n0         1           0.97           9.3\n1         2           1.10          14.3\n2         1           0.20          12.3\n\nAttempting to read from https://storage.googleapis.com/cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS (via HTTPS)!\n         name post_abbr\n0     Alabama        AL\n1      Alaska        AK\n2     Arizona        AZ\n3    Arkansas        AR\n4  California        CA\n"
  },
  "transform_filter_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    # Mock data for demonstration\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (Boolean Indexing)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\nadelie_df = df[df['species'] == 'Adelie']\nprint(\"\\n--- Filter: Species == 'Adelie' ---\")\nprint(adelie_df.head(3))\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\n# Note: Parentheses are mandatory for multiple conditions in Pandas!\ncondition = (df['species'] == 'Adelie') & (df['island'] == 'Torgersen')\ntorgersen_adelie = df[condition]\nprint(\"\\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\")\nprint(torgersen_adelie.head(3))\n\n# ---------------------------------------------------------\n# 3. Using .query() (String Syntax)\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nlong_bills = df.query(\"bill_length_mm > 45\")\nprint(\"\\n--- Filter: bill_length_mm > 45 (using .query()) ---\")\nprint(long_bills.head(3))\n\n# ---------------------------------------------------------\n# 4. String Matching\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\nbiscoe_df = df[df['island'].str.startswith('B')]\nprint(\"\\n--- Filter: Island starts with 'B' ---\")\nprint(biscoe_df.head(3))\n\n# ---------------------------------------------------------\n# 5. Null Handling\n# ---------------------------------------------------------\n# Drop rows where 'sex' is missing\nclean_df = df.dropna(subset=['sex'])\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nprint(f\"Original: {len(df)}, Cleaned: {len(clean_df)}\")",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Filter: Species == 'Adelie' ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Filter: bill_length_mm > 45 (using .query()) ---\n    species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g   sex\n19   Adelie  Torgersen            46.0  ...              194.0       4200.0  MALE\n73   Adelie  Torgersen            45.8  ...              197.0       4150.0  MALE\n111  Adelie     Biscoe            45.6  ...              191.0       4600.0  MALE\n\n[3 rows x 7 columns]\n\n--- Filter: Island starts with 'B' ---\n   species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n20  Adelie  Biscoe            37.8  ...              174.0       3400.0  FEMALE\n21  Adelie  Biscoe            37.7  ...              180.0       3600.0    MALE\n22  Adelie  Biscoe            35.9  ...              189.0       3800.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344, Cleaned: 333\nInstalled 4 packages in 65ms\n"
  },
  "transform_filter_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"fsspec\",\n#     \"requests\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    # Polars can read from URL directly\n    df = pl.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (pl.col expression)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\n# In Polars, .filter() is the primary method\nadelie_df = df.filter(pl.col(\"species\") == \"Adelie\")\nprint(\"\\n--- Filter: Species == 'Adelie' ---\")\nprint(adelie_df.head(3))\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\n# Use & for AND, | for OR. Parentheses are required!\ntorgersen_adelie = df.filter(\n    (pl.col(\"species\") == \"Adelie\") & \n    (pl.col(\"island\") == \"Torgersen\")\n)\nprint(\"\\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\")\nprint(torgersen_adelie.head(3))\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nlong_bills = df.filter(pl.col(\"bill_length_mm\") > 45)\nprint(\"\\n--- Filter: bill_length_mm > 45 ---\")\nprint(long_bills.head(3))\n\n# ---------------------------------------------------------\n# 4. String Matching\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\nbiscoe_df = df.filter(pl.col(\"island\").str.starts_with(\"B\"))\nprint(\"\\n--- Filter: Island starts with 'B' ---\")\nprint(biscoe_df.head(3))\n\n# ---------------------------------------------------------\n# 5. Null Handling\n# ---------------------------------------------------------\n# Drop rows where 'sex' is null\nclean_df = df.drop_nulls(subset=[\"sex\"])\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nprint(f\"Original: {len(df)}, Cleaned: {len(clean_df)}\")",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Filter: Species == 'Adelie' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: bill_length_mm > 45 ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex  \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---  \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 46.0           \u2506 21.5          \u2506 194               \u2506 4200        \u2506 MALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 45.8           \u2506 18.9          \u2506 197               \u2506 4150        \u2506 MALE \u2502\n\u2502 Adelie  \u2506 Biscoe    \u2506 45.6           \u2506 20.3          \u2506 191               \u2506 4600        \u2506 MALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: Island starts with 'B' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---    \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str    \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Biscoe \u2506 37.8           \u2506 18.3          \u2506 174               \u2506 3400        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Biscoe \u2506 37.7           \u2506 18.7          \u2506 180               \u2506 3600        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Biscoe \u2506 35.9           \u2506 19.2          \u2506 189               \u2506 3800        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344, Cleaned: 333\nInstalled 8 packages in 20ms\n"
  },
  "transform_filter_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nTABLE_NAME = \"penguins\"\n\ntry:\n    # DuckDB handles HTTPS if httpfs is installed/loaded (auto-loaded in modern versions)\n    # We create a view or table to query against easily\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM '{URL}'\")\n    print(f\"Loaded data from {URL} into DuckDB table '{TABLE_NAME}'\")\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data via Pandas.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    # Create a Pandas DataFrame\n    pandas_df = pd.read_csv(io.StringIO(data))\n    # DuckDB can query Pandas DataFrames directly!\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM pandas_df\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (SQL WHERE)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\nprint(\"\\n--- Filter: Species = 'Adelie' ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE species = 'Adelie' LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions (AND/OR)\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\nprint(\"\\n--- Filter: Species = 'Adelie' AND Island = 'Torgersen' ---\")\nduckdb.sql(f\"\"\"\n    SELECT * FROM {TABLE_NAME} \n    WHERE species = 'Adelie' AND island = 'Torgersen' \n    LIMIT 3\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nprint(\"\\n--- Filter: bill_length_mm > 45 ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE bill_length_mm > 45 LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 4. String Matching (LIKE/ILIKE)\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\n# '%' is the wildcard in SQL\nprint(\"\\n--- Filter: Island LIKE 'B%' ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE island LIKE 'B%' LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 5. Null Handling (IS NOT NULL)\n# ---------------------------------------------------------\n# Drop rows where 'sex' is null\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nresult = duckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE sex IS NOT NULL\")\nprint(f\"Original: {duckdb.sql(f'SELECT count(*) FROM {TABLE_NAME}').fetchone()[0]}\")\nprint(f\"Cleaned: {result.count('*').fetchone()[0]}\")",
    "language": "python",
    "output": "Loaded data from https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv into DuckDB table 'penguins'\n\n--- Filter: Species = 'Adelie' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: Species = 'Adelie' AND Island = 'Torgersen' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: bill_length_mm > 45 ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           46.0 \u2502          21.5 \u2502               194 \u2502        4200 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           45.8 \u2502          18.9 \u2502               197 \u2502        4150 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Biscoe    \u2502           45.6 \u2502          20.3 \u2502               191 \u2502        4600 \u2502 MALE    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: Island LIKE 'B%' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502 island  \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502 varchar \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Biscoe  \u2502           37.8 \u2502          18.3 \u2502               174 \u2502        3400 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Biscoe  \u2502           37.7 \u2502          18.7 \u2502               180 \u2502        3600 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Biscoe  \u2502           35.9 \u2502          19.2 \u2502               189 \u2502        3800 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344\nCleaned: 333\nInstalled 5 packages in 59ms\n"
  },
  "transform_filter_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"db-dtypes\",\n# ]\n# ///\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Mock Setup (Simulating BigQuery)\n# ---------------------------------------------------------\n# In a real scenario, you would use:\n# client = bigquery.Client()\n\nmock_client = mock.MagicMock(spec=bigquery.Client)\nprint(\"--- BigQuery Client Initialized (Mock) ---\\n\")\n\n# Load mock data for results\ndata = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\n\"\"\"\ndf_mock = pd.read_csv(io.StringIO(data))\n\ndef mock_query(query):\n    print(f\"Executing SQL:\\n{query}\\n\")\n    \n    # We'll just filter the mock dataframe to simulate the query result\n    # ensuring the output matches the SQL intent roughly\n    \n    result = df_mock.copy()\n    \n    if \"species = 'Adelie'\" in query and \"island = 'Torgersen'\" in query:\n        result = result[(result['species'] == 'Adelie') & (result['island'] == 'Torgersen')]\n    elif \"species = 'Adelie'\" in query:\n        result = result[result['species'] == 'Adelie']\n    elif \"bill_length_mm > 45\" in query:\n        result = result[result['bill_length_mm'] > 45]\n    elif \"island LIKE 'B%'\" in query:\n        result = result[result['island'].str.startswith('B')]\n    \n    # Mock row iterator\n    mock_job = mock.MagicMock()\n    mock_job.to_dataframe.return_value = result.head(3)\n    return mock_job\n\nmock_client.query.side_effect = mock_query\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (SQL WHERE)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions (AND/OR)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie' AND island = 'Torgersen'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE bill_length_mm > 45\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 4. String Matching (LIKE)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE island LIKE 'B%'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie'\n    LIMIT 3\n\n\n--- Result ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...                181         3750    Male\n1  Adelie  Torgersen            39.5  ...                186         3800  Female\n2  Adelie  Torgersen            40.3  ...                195         3250  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie' AND island = 'Torgersen'\n    LIMIT 3\n\n\n\n--- Result ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...                181         3750    Male\n1  Adelie  Torgersen            39.5  ...                186         3800  Female\n2  Adelie  Torgersen            40.3  ...                195         3250  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE bill_length_mm > 45\n    LIMIT 3\n\n\n\n--- Result ---\n     species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n5  Chinstrap   Dream            46.5  ...                192         3500  Female\n6     Gentoo  Biscoe            46.1  ...                211         4500  Female\n\n[2 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE island LIKE 'B%'\n    LIMIT 3\n\n\n\n--- Result ---\n  species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n6  Gentoo  Biscoe            46.1  ...                211         4500  Female\n\n[1 rows x 7 columns]\nInstalled 30 packages in 118ms\n"
  }
}
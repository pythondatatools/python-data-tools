{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading\n# We use the standardized penguins.csv dataset\ndf = pd.read_csv(DATA_PATH)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options (Selective columns and handling NaNs)\ndf = pd.read_csv(\n    DATA_PATH,\n    usecols=[\"species\", \"island\", \"bill_length_mm\"],\n    na_values=[\"NA\"],\n)\nprint(\"\\nSelective columns:\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded 344 rows:\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nSelective columns:\n  species     island  bill_length_mm\n0  Adelie  Torgersen            39.1\n1  Adelie  Torgersen            39.5\n2  Adelie  Torgersen            40.3\n3  Adelie  Torgersen             NaN\n4  Adelie  Torgersen            36.7\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading (eager)\n# Polars is extremely fast at CSV parsing\ndf = pl.read_csv(DATA_PATH)\nprint(\"Polars loaded penguins dataset:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(DATA_PATH)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded penguins dataset:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193               \u2506 3450        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading via SQL\n# DuckDB can query CSV files directly\nprint(\"DuckDB querying penguins via SQL:\")\nduckdb.sql(f\"SELECT * FROM '{DATA_PATH}' LIMIT 5\").show()",
    "language": "python",
    "output": "DuckDB querying penguins via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502               193 \u2502        3450 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.penguins\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()  # Wait for the job to complete\n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.penguins\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet\n# Pandas uses the pyarrow engine by default for Parquet\ndf = pd.read_parquet(DATA_PATH)\nprint(f\"Pandas loaded penguins Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded penguins Parquet with 344 rows.\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet (Eager)\n# Polars is built on Apache Arrow for ultra-fast Parquet performance\ndf = pl.read_parquet(DATA_PATH)\nprint(\"Polars loaded penguins Parquet:\")\nprint(df.head())\n\n# Scan Parquet (Lazy - Recommended for Large Data)\n# This allows Polars to skip reading columns/rows not needed\nquery = pl.scan_parquet(DATA_PATH).select([\"species\", \"island\"]).limit(5)\nprint(\"\\nLazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded penguins Parquet:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLazy scan result:\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2502\n\u2502 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to Parquet\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{DATA_PATH}' (FORMAT PARQUET)\")\n\n# %%\n# Query Parquet directly via SQL\n# DuckDB treats Parquet files as virtual tables\nprint(\"DuckDB querying penguins Parquet via SQL:\")\nduckdb.sql(f\"SELECT species, island FROM '{DATA_PATH}' LIMIT 5\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(DATA_PATH))\nprint(\"\\nRead as relation (First 5):\")\nrel.limit(5).show()",
    "language": "python",
    "output": "DuckDB querying penguins Parquet via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nRead as relation (First 5):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing (Local mock data)\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    import pandas as pd # Needed for conversion\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.penguins_parquet\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\n# Parquet is the recommended format for BigQuery due to schema persistence\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.penguins_parquet\n"
  },
  "load_json_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Pandas\n# Handling standard JSON and Newline Delimited JSON (JSONL).\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    df_temp = pd.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.to_json(JSON_PATH, orient=\"records\")\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Load standard JSON\ndf_json = pd.read_json(JSON_PATH)\nprint(\"Standard JSON (Pandas):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (lines=True)\ndf_ndjson = pd.read_json(NDJSON_PATH, lines=True)\nprint(\"\\nNDJSON (lines=True):\")\nprint(df_ndjson.head())",
    "language": "python",
    "output": "Standard JSON (Pandas):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nNDJSON (lines=True):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_json_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Polars\n# Fast parsing for structured and semi-structured data.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    \n    # Use Polars itself for high-performance conversion\n    df_temp = pl.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.write_json(JSON_PATH)\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.write_ndjson(NDJSON_PATH)\n\n# %%\n# Load standard JSON (Eager)\n# Polars parses standard JSON into memory efficiently\ndf_json = pl.read_json(JSON_PATH)\nprint(\"Standard JSON (Polars):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (Fastest)\n# Newline Delimited JSON is the preferred format for high-speed Polars loading\ndf_ndjson = pl.read_ndjson(NDJSON_PATH)\nprint(\"\\nNDJSON (Polars):\")\nprint(df_ndjson.head())\n\n# Scan NDJSON (Lazy - Great for large logs)\n# df_lazy = pl.scan_ndjson(ndjson_path).collect()",
    "language": "python",
    "output": "Standard JSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNDJSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_json_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with DuckDB\n# Direct SQL queries on JSON and NDJSON files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to JSON and NDJSON\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{JSON_PATH}' (FORMAT JSON, ARRAY TRUE)\")\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{NDJSON_PATH}' (FORMAT JSON)\")\n\n# %%\n# Query JSON directly via SQL\n# DuckDB treats JSON files as virtual tables with auto-schema detection\nprint(\"Standard JSON via SQL:\")\nduckdb.sql(f\"SELECT species, island, island FROM read_json_auto('{JSON_PATH}') LIMIT 5\").show()\n\n# Query NDJSON\nprint(\"\\nNDJSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{NDJSON_PATH}') LIMIT 5\").show()",
    "language": "python",
    "output": "Standard JSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nNDJSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_json_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with BigQuery\n# Loading NDJSON data into BigQuery tables. Note: BigQuery requires NDJSON (Newline Delimited) for loading local files.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert to NDJSON (Native to BigQuery load)\nif not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\ntable_id = \"project.dataset.penguins_json\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n    autodetect=True,\n)\n\n# %%\n# Load NDJSON to BigQuery\n# BigQuery requires Newline Delimited JSON (NDJSON) for direct file loads\nwith open(NDJSON_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery JSON load (NDJSON) triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery JSON load (NDJSON) triggered for project.dataset.penguins_json\n"
  },
  "load_database_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists (Self-healing)\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        # Create dummy data\n        df_dummy = pd.DataFrame({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n            \"dept\": [\"HR\", \"IT\", \"IT\", \"Finance\", \"HR\"],\n            \"salary\": [60000, 80000, 75000, 90000, 62000]\n        })\n        df_dummy.to_sql(\"employees\", conn, index=False)\n\n# 2. Main: Read from Database\n# Use a context manager to ensure connection closes\nwith sqlite3.connect(db_path) as conn:\n    # Read entire table\n    df = pd.read_sql(\"SELECT * FROM employees\", conn)\n    print(\"--- All Employees ---\")\n    print(df.head())\n    \n    # Read with filter query\n    query = \"SELECT name, salary FROM employees WHERE dept = 'IT'\"\n    df_it = pd.read_sql(query, conn)\n    print(\"\\n--- IT Department ---\")\n    print(df_it)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (requires psycopg2)\n# conn_str = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# MS SQL Server (requires pyodbc)\n# conn_str = \"mssql+pyodbc://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# IBM DB2 (requires ibm_db_sa)\n# conn_str = \"ibm_db_sa://user:password@host:port/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)",
    "language": "python",
    "output": "--- All Employees ---\n   id     name     dept  salary\n0   1    Alice       HR   60000\n1   2      Bob       IT   80000\n2   3  Charlie       IT   75000\n3   4    David  Finance   90000\n4   5      Eve       HR   62000\n\n--- IT Department ---\n      name  salary\n0      Bob   80000\n1  Charlie   75000\n"
  },
  "load_database_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"connectorx\",\n#     \"pyarrow\", \n# ]\n# ///\nimport polars as pl\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n        conn.execute(\"INSERT INTO employees VALUES (3, 'Charlie', 'IT', 75000)\")\n\n# 2. Main: Read from Database\n# Polars uses connectorx or adbc for high performance\nuri = f\"sqlite://{db_path}\"\n\nquery = \"SELECT * FROM employees\"\ndf = pl.read_database_uri(query=query, uri=uri)\n\nprint(\"--- Polars DataFrame ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (High Permance via ConnectorX)\n# uri = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# MS SQL Server (High Performance via ConnectorX)\n# uri = \"mssql://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# IBM DB2 (via SQLAlchemy fallback)\n# import sqlalchemy\n# engine = sqlalchemy.create_engine(\"ibm_db_sa://user:password@host:port/mydb\")\n# df = pl.read_database(\"SELECT * FROM employees\", connection=engine)",
    "language": "python",
    "output": "--- Polars DataFrame ---\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept    \u2506 salary \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2506 ---    \u2502\n\u2502 i64 \u2506 str     \u2506 str     \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 HR      \u2506 60000  \u2502\n\u2502 2   \u2506 Bob     \u2506 IT      \u2506 80000  \u2502\n\u2502 3   \u2506 Charlie \u2506 IT      \u2506 75000  \u2502\n\u2502 4   \u2506 David   \u2506 Finance \u2506 90000  \u2502\n\u2502 5   \u2506 Eve     \u2506 HR      \u2506 62000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_database_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\nimport duckdb\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n\n# 2. Main: Read from Database\n# DuckDB can query SQLite files directly without importing!\n# This is \"zero-copy\" - it reads the file format directly.\n\n# Install/Load sqlite extension (handled automatically by modern DuckDB, but good to know)\nduckdb.sql(\"INSTALL sqlite; LOAD sqlite;\")\n\n# Query directly\nquery = f\"SELECT * FROM sqlite_scan('{db_path}', 'employees')\"\ndf = duckdb.sql(query).show()\n\n# You can also attach it as a database\nprint(\"\\n--- Attached Database ---\")\nduckdb.sql(f\"ATTACH '{db_path}' AS mydb (TYPE SQLITE)\")\nduckdb.sql(\"SELECT name, salary FROM mydb.employees WHERE salary > 70000\").show()\n\n# ---------------------------------------------------------\n# PRO TIP: Enterprise Data Federation\n# ---------------------------------------------------------\n# PostgreSQL\n# duckdb.sql(\"INSTALL postgres; LOAD postgres;\")\n# duckdb.sql(\"ATTACH 'dbname=mydb user=user password=pass host=localhost' AS my_pg (TYPE POSTGRES)\")\n# duckdb.sql(\"SELECT * FROM my_pg.employees\").show()\n\n# MySQL / MariaDB\n# duckdb.sql(\"INSTALL mysql; LOAD mysql;\")\n# duckdb.sql(\"ATTACH 'user=user password=pass database=mydb' AS my_mysql (TYPE MYSQL)\")\n# duckdb.sql(\"SELECT * FROM my_mysql.employees\").show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502  dept   \u2502 salary \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502 HR      \u2502  60000 \u2502\n\u2502     2 \u2502 Bob     \u2502 IT      \u2502  80000 \u2502\n\u2502     3 \u2502 Charlie \u2502 IT      \u2502  75000 \u2502\n\u2502     4 \u2502 David   \u2502 Finance \u2502  90000 \u2502\n\u2502     5 \u2502 Eve     \u2502 HR      \u2502  62000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Attached Database ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502 salary \u2502\n\u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Bob     \u2502  80000 \u2502\n\u2502 Charlie \u2502  75000 \u2502\n\u2502 David   \u2502  90000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_database_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"db-dtypes\",\n#     \"pyarrow\", \n# ]\n# ///\nfrom google.cloud import bigquery\nimport os\n\n# Note: BigQuery cannot read your local SQLite file.\n# In production, you would use \"Federated Queries\" (EXTERNAL_QUERY) \n# to query a Cloud SQL (Postgres/MySQL) database without moving data.\n\n# Pseudo-code for simulation since we don't have active credentials\nprint(\"--- BigQuery Federated Query ---\")\n\nsql = \"\"\"\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\"\"\"\n\nprint(f\"Executing Query:\\n{sql}\")\n\n# In a real environment:\n# client = bigquery.Client()\n# df = client.query(sql).to_dataframe()\n# print(df)\n\nprint(\"\\n[Simulation] Result DataFrame:\")\nprint(\"   id     name dept  salary\")\nprint(\"0   2      Bob   IT   80000\")\nprint(\"1   3  Charlie   IT   75000\")",
    "language": "python",
    "output": "--- BigQuery Federated Query ---\nExecuting Query:\n\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\n\n[Simulation] Result DataFrame:\n   id     name dept  salary\n0   2      Bob   IT   80000\n1   3  Charlie   IT   75000\n"
  }
}
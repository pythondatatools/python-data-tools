{
  "load_csv_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Pandas\n# This example demonstrates how to load a CSV file, with self-healing data generation.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading\n# We use the standardized penguins.csv dataset\ndf = pd.read_csv(DATA_PATH)\nprint(f\"Pandas loaded {len(df)} rows:\")\nprint(df.head())\n\n# With options (Selective columns and handling NaNs)\ndf = pd.read_csv(\n    DATA_PATH,\n    usecols=[\"species\", \"island\", \"bill_length_mm\"],\n    na_values=[\"NA\"],\n)\nprint(\"\\nSelective columns:\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded 344 rows:\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nSelective columns:\n  species     island  bill_length_mm\n0  Adelie  Torgersen            39.1\n1  Adelie  Torgersen            39.5\n2  Adelie  Torgersen            40.3\n3  Adelie  Torgersen             NaN\n4  Adelie  Torgersen            36.7\n"
  },
  "load_csv_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with Polars\n# Demonstrates high-performance CSV loading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading (eager)\n# Polars is extremely fast at CSV parsing\ndf = pl.read_csv(DATA_PATH)\nprint(\"Polars loaded penguins dataset:\")\nprint(df.head())\n\n# Lazy loading (recommended for large files)\nlf = pl.scan_csv(DATA_PATH)\ndf = lf.collect()",
    "language": "python",
    "output": "Polars loaded penguins dataset:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193               \u2506 3450        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_csv_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with DuckDB\n# Using SQL to query CSV files directly.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Basic CSV loading via SQL\n# DuckDB can query CSV files directly\nprint(\"DuckDB querying penguins via SQL:\")\nduckdb.sql(f\"SELECT * FROM '{DATA_PATH}' LIMIT 5\").show()",
    "language": "python",
    "output": "DuckDB querying penguins via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502               193 \u2502        3450 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_csv_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### CSV Loading with BigQuery\n# Demonstrates loading local data to BigQuery.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.csv\")\n\n# Self-healing: Download if missing\nif not DATA_PATH.exists():\n    urllib.request.urlretrieve(CSV_URL, DATA_PATH)\n\n# %%\n# Mock the client for CI/verification purposes\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Initialize client (Mocked for verification)\n# client = bigquery.Client()\n\n# Load CSV from local file to BigQuery table\ntable_id = \"project.dataset.penguins\"\n\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n# %%\n# Mocking the load_table_from_file behavior\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()  # Wait for the job to complete\n\nprint(f\"Mock BigQuery load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery load triggered for project.dataset.penguins\n"
  },
  "load_parquet_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Pandas\n# Columnar data handling with PyArrow engine.\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet\n# Pandas uses the pyarrow engine by default for Parquet\ndf = pd.read_parquet(DATA_PATH)\nprint(f\"Pandas loaded penguins Parquet with {len(df)} rows.\")\nprint(df.head())",
    "language": "python",
    "output": "Pandas loaded penguins Parquet with 344 rows.\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_parquet_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with Polars\n# Fast, multithreaded Parquet reading.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Load Parquet (Eager)\n# Polars is built on Apache Arrow for ultra-fast Parquet performance\ndf = pl.read_parquet(DATA_PATH)\nprint(\"Polars loaded penguins Parquet:\")\nprint(df.head())\n\n# Scan Parquet (Lazy - Recommended for Large Data)\n# This allows Polars to skip reading columns/rows not needed\nquery = pl.scan_parquet(DATA_PATH).select([\"species\", \"island\"]).limit(5)\nprint(\"\\nLazy scan result:\")\nprint(query.collect())",
    "language": "python",
    "output": "Polars loaded penguins Parquet:\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLazy scan result:\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2502\n\u2502 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2502 Adelie  \u2506 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_parquet_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with DuckDB\n# Direct SQL queries on Parquet files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to Parquet\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{DATA_PATH}' (FORMAT PARQUET)\")\n\n# %%\n# Query Parquet directly via SQL\n# DuckDB treats Parquet files as virtual tables\nprint(\"DuckDB querying penguins Parquet via SQL:\")\nduckdb.sql(f\"SELECT species, island FROM '{DATA_PATH}' LIMIT 5\").show()\n\n# Read as relation\nrel = duckdb.read_parquet(str(DATA_PATH))\nprint(\"\\nRead as relation (First 5):\")\nrel.limit(5).show()",
    "language": "python",
    "output": "DuckDB querying penguins Parquet via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nRead as relation (First 5):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_parquet_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\n# %% [markdown]\n# ### Parquet Loading with BigQuery\n# Demonstrates loading Parquet data into BigQuery tables.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nDATA_PATH = pathlib.Path(\"penguins.parquet\")\n\n# Self-healing: Download and convert if missing (Local mock data)\nif not DATA_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    import pandas as pd # Needed for conversion\n    pd.read_csv(csv_temp).to_parquet(DATA_PATH)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\n# Load configuration\ntable_id = \"project.dataset.penguins_parquet\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.PARQUET,\n    write_disposition=\"WRITE_TRUNCATE\",\n)\n\n# %%\n# Trigger load\n# Parquet is the recommended format for BigQuery due to schema persistence\nwith open(DATA_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery Parquet load triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery Parquet load triggered for project.dataset.penguins_parquet\n"
  },
  "load_json_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Pandas\n# Handling standard JSON and Newline Delimited JSON (JSONL).\n\n# %%\nimport pandas as pd\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    df_temp = pd.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.to_json(JSON_PATH, orient=\"records\")\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Load standard JSON\ndf_json = pd.read_json(JSON_PATH)\nprint(\"Standard JSON (Pandas):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (lines=True)\ndf_ndjson = pd.read_json(NDJSON_PATH, lines=True)\nprint(\"\\nNDJSON (lines=True):\")\nprint(df_ndjson.head())",
    "language": "python",
    "output": "Standard JSON (Pandas):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n\nNDJSON (lines=True):\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN  ...                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  FEMALE\n\n[5 rows x 7 columns]\n"
  },
  "load_json_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON & NDJSON Loading with Polars\n# Fast parsing for structured and semi-structured data.\n\n# %%\nimport polars as pl\nimport pathlib\nimport urllib.request\nimport json\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    \n    # Use Polars itself for high-performance conversion\n    df_temp = pl.read_csv(csv_temp)\n    \n    # Save as standard JSON (Array of objects)\n    df_temp.write_json(JSON_PATH)\n    \n    # Save as NDJSON (Newline Delimited)\n    df_temp.write_ndjson(NDJSON_PATH)\n\n# %%\n# Load standard JSON (Eager)\n# Polars parses standard JSON into memory efficiently\ndf_json = pl.read_json(JSON_PATH)\nprint(\"Standard JSON (Polars):\")\nprint(df_json.head())\n\n# %%\n# Load NDJSON (Fastest)\n# Newline Delimited JSON is the preferred format for high-speed Polars loading\ndf_ndjson = pl.read_ndjson(NDJSON_PATH)\nprint(\"\\nNDJSON (Polars):\")\nprint(df_ndjson.head())\n\n# Scan NDJSON (Lazy - Great for large logs)\n# df_lazy = pl.scan_ndjson(ndjson_path).collect()",
    "language": "python",
    "output": "Standard JSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNDJSON (Polars):\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 f64               \u2506 f64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181.0             \u2506 3750.0      \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186.0             \u2506 3800.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195.0             \u2506 3250.0      \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 null           \u2506 null          \u2506 null              \u2506 null        \u2506 null   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 36.7           \u2506 19.3          \u2506 193.0             \u2506 3450.0      \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_json_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with DuckDB\n# Direct SQL queries on JSON and NDJSON files.\n\n# %%\nimport duckdb\nimport pathlib\nimport urllib.request\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nJSON_PATH = pathlib.Path(\"penguins.json\")\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert if missing\nif not JSON_PATH.exists() or not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    # Use DuckDB itself to convert CSV to JSON and NDJSON\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{JSON_PATH}' (FORMAT JSON, ARRAY TRUE)\")\n    duckdb.sql(f\"COPY (SELECT * FROM read_csv_auto('{csv_temp}')) TO '{NDJSON_PATH}' (FORMAT JSON)\")\n\n# %%\n# Query JSON directly via SQL\n# DuckDB treats JSON files as virtual tables with auto-schema detection\nprint(\"Standard JSON via SQL:\")\nduckdb.sql(f\"SELECT species, island, island FROM read_json_auto('{JSON_PATH}') LIMIT 5\").show()\n\n# Query NDJSON\nprint(\"\\nNDJSON via SQL:\")\nduckdb.sql(f\"SELECT * FROM read_json_auto('{NDJSON_PATH}') LIMIT 5\").show()",
    "language": "python",
    "output": "Standard JSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502  island   \u2502\n\u2502 varchar \u2502  varchar  \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502 Torgersen \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nNDJSON via SQL:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502      double       \u2502   double    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502             181.0 \u2502      3750.0 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502             186.0 \u2502      3800.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502             195.0 \u2502      3250.0 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           NULL \u2502          NULL \u2502              NULL \u2502        NULL \u2502 NULL    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           36.7 \u2502          19.3 \u2502             193.0 \u2502      3450.0 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_json_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\n# %% [markdown]\n# ### JSON Loading with BigQuery\n# Loading NDJSON data into BigQuery tables. Note: BigQuery requires NDJSON (Newline Delimited) for loading local files.\n\n# %%\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pathlib\nimport urllib.request\nimport pandas as pd # For initial conversion\n\n# Standard \"Wrangling Hero\" dataset: Palmer Penguins\nCSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nNDJSON_PATH = pathlib.Path(\"penguins.jsonl\")\n\n# Self-healing: Download and convert to NDJSON (Native to BigQuery load)\nif not NDJSON_PATH.exists():\n    csv_temp = pathlib.Path(\"penguins.csv\")\n    if not csv_temp.exists():\n        urllib.request.urlretrieve(CSV_URL, csv_temp)\n    pd.read_csv(csv_temp).to_json(NDJSON_PATH, orient=\"records\", lines=True)\n\n# %%\n# Mock the client\nclient = mock.MagicMock(spec=bigquery.Client)\n\ntable_id = \"project.dataset.penguins_json\"\njob_config = bigquery.LoadJobConfig(\n    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n    autodetect=True,\n)\n\n# %%\n# Load NDJSON to BigQuery\n# BigQuery requires Newline Delimited JSON (NDJSON) for direct file loads\nwith open(NDJSON_PATH, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n    job.result()\n\nprint(f\"Mock BigQuery JSON load (NDJSON) triggered for {table_id}\")",
    "language": "python",
    "output": "Mock BigQuery JSON load (NDJSON) triggered for project.dataset.penguins_json\n"
  },
  "load_database_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists (Self-healing)\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        # Create dummy data\n        df_dummy = pd.DataFrame({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n            \"dept\": [\"HR\", \"IT\", \"IT\", \"Finance\", \"HR\"],\n            \"salary\": [60000, 80000, 75000, 90000, 62000]\n        })\n        df_dummy.to_sql(\"employees\", conn, index=False)\n\n# 2. Main: Read from Database\n# Use a context manager to ensure connection closes\nwith sqlite3.connect(db_path) as conn:\n    # Read entire table\n    df = pd.read_sql(\"SELECT * FROM employees\", conn)\n    print(\"--- All Employees ---\")\n    print(df.head())\n    \n    # Read with filter query\n    query = \"SELECT name, salary FROM employees WHERE dept = 'IT'\"\n    df_it = pd.read_sql(query, conn)\n    print(\"\\n--- IT Department ---\")\n    print(df_it)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (requires psycopg2)\n# conn_str = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# MS SQL Server (requires pyodbc)\n# conn_str = \"mssql+pyodbc://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)\n\n# IBM DB2 (requires ibm_db_sa)\n# conn_str = \"ibm_db_sa://user:password@host:port/mydb\"\n# df = pd.read_sql(\"SELECT * FROM employees\", conn_str)",
    "language": "python",
    "output": "--- All Employees ---\n   id     name     dept  salary\n0   1    Alice       HR   60000\n1   2      Bob       IT   80000\n2   3  Charlie       IT   75000\n3   4    David  Finance   90000\n4   5      Eve       HR   62000\n\n--- IT Department ---\n      name  salary\n0      Bob   80000\n1  Charlie   75000\n"
  },
  "load_database_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"connectorx\",\n#     \"pyarrow\", \n# ]\n# ///\nimport polars as pl\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n        conn.execute(\"INSERT INTO employees VALUES (3, 'Charlie', 'IT', 75000)\")\n\n# 2. Main: Read from Database\n# Polars uses connectorx or adbc for high performance\nuri = f\"sqlite://{db_path}\"\n\nquery = \"SELECT * FROM employees\"\ndf = pl.read_database_uri(query=query, uri=uri)\n\nprint(\"--- Polars DataFrame ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# PRO TIP: Connecting to Enterprise Databases\n# ---------------------------------------------------------\n# PostgreSQL (High Permance via ConnectorX)\n# uri = \"postgresql://user:password@localhost:5432/mydb\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# MS SQL Server (High Performance via ConnectorX)\n# uri = \"mssql://user:password@server/mydb?driver=ODBC+Driver+17+for+SQL+Server\"\n# df = pl.read_database_uri(\"SELECT * FROM employees\", uri)\n\n# IBM DB2 (via SQLAlchemy fallback)\n# import sqlalchemy\n# engine = sqlalchemy.create_engine(\"ibm_db_sa://user:password@host:port/mydb\")\n# df = pl.read_database(\"SELECT * FROM employees\", connection=engine)",
    "language": "python",
    "output": "--- Polars DataFrame ---\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept    \u2506 salary \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2506 ---    \u2502\n\u2502 i64 \u2506 str     \u2506 str     \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 HR      \u2506 60000  \u2502\n\u2502 2   \u2506 Bob     \u2506 IT      \u2506 80000  \u2502\n\u2502 3   \u2506 Charlie \u2506 IT      \u2506 75000  \u2502\n\u2502 4   \u2506 David   \u2506 Finance \u2506 90000  \u2502\n\u2502 5   \u2506 Eve     \u2506 HR      \u2506 62000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_database_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\nimport duckdb\nimport sqlite3\nimport pathlib\n\n# 1. Setup: Ensure database exists\ndb_path = \"my_database.db\"\nif not pathlib.Path(db_path).exists():\n    print(f\"Creating {db_path}...\")\n    with sqlite3.connect(db_path) as conn:\n        conn.execute(\"CREATE TABLE employees (id INT, name TEXT, dept TEXT, salary INT)\")\n        conn.execute(\"INSERT INTO employees VALUES (1, 'Alice', 'HR', 60000)\")\n        conn.execute(\"INSERT INTO employees VALUES (2, 'Bob', 'IT', 80000)\")\n\n# 2. Main: Read from Database\n# DuckDB can query SQLite files directly without importing!\n# This is \"zero-copy\" - it reads the file format directly.\n\n# Install/Load sqlite extension (handled automatically by modern DuckDB, but good to know)\nduckdb.sql(\"INSTALL sqlite; LOAD sqlite;\")\n\n# Query directly\nquery = f\"SELECT * FROM sqlite_scan('{db_path}', 'employees')\"\ndf = duckdb.sql(query).show()\n\n# You can also attach it as a database\nprint(\"\\n--- Attached Database ---\")\nduckdb.sql(f\"ATTACH '{db_path}' AS mydb (TYPE SQLITE)\")\nduckdb.sql(\"SELECT name, salary FROM mydb.employees WHERE salary > 70000\").show()\n\n# ---------------------------------------------------------\n# PRO TIP: Enterprise Data Federation\n# ---------------------------------------------------------\n# PostgreSQL\n# duckdb.sql(\"INSTALL postgres; LOAD postgres;\")\n# duckdb.sql(\"ATTACH 'dbname=mydb user=user password=pass host=localhost' AS my_pg (TYPE POSTGRES)\")\n# duckdb.sql(\"SELECT * FROM my_pg.employees\").show()\n\n# MySQL / MariaDB\n# duckdb.sql(\"INSTALL mysql; LOAD mysql;\")\n# duckdb.sql(\"ATTACH 'user=user password=pass database=mydb' AS my_mysql (TYPE MYSQL)\")\n# duckdb.sql(\"SELECT * FROM my_mysql.employees\").show()",
    "language": "python",
    "output": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502  dept   \u2502 salary \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502 HR      \u2502  60000 \u2502\n\u2502     2 \u2502 Bob     \u2502 IT      \u2502  80000 \u2502\n\u2502     3 \u2502 Charlie \u2502 IT      \u2502  75000 \u2502\n\u2502     4 \u2502 David   \u2502 Finance \u2502  90000 \u2502\n\u2502     5 \u2502 Eve     \u2502 HR      \u2502  62000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Attached Database ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502 salary \u2502\n\u2502 varchar \u2502 int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Bob     \u2502  80000 \u2502\n\u2502 Charlie \u2502  75000 \u2502\n\u2502 David   \u2502  90000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "load_database_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"db-dtypes\",\n#     \"pyarrow\", \n# ]\n# ///\nfrom google.cloud import bigquery\nimport os\n\n# Note: BigQuery cannot read your local SQLite file.\n# In production, you would use \"Federated Queries\" (EXTERNAL_QUERY) \n# to query a Cloud SQL (Postgres/MySQL) database without moving data.\n\n# Pseudo-code for simulation since we don't have active credentials\nprint(\"--- BigQuery Federated Query ---\")\n\nsql = \"\"\"\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\"\"\"\n\nprint(f\"Executing Query:\\n{sql}\")\n\n# In a real environment:\n# client = bigquery.Client()\n# df = client.query(sql).to_dataframe()\n# print(df)\n\nprint(\"\\n[Simulation] Result DataFrame:\")\nprint(\"   id     name dept  salary\")\nprint(\"0   2      Bob   IT   80000\")\nprint(\"1   3  Charlie   IT   75000\")",
    "language": "python",
    "output": "--- BigQuery Federated Query ---\nExecuting Query:\n\nSELECT * FROM EXTERNAL_QUERY(\n    'projects/my-project/locations/us/connections/my-connection',\n    'SELECT * FROM employees WHERE dept = \"IT\"'\n);\n\n\n[Simulation] Result DataFrame:\n   id     name dept  salary\n0   2      Bob   IT   80000\n1   3  Charlie   IT   75000\n"
  },
  "load_cloud_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"s3fs\",\n#     \"gcsfs\",\n#     \"pyarrow\",\n# ]\n# ///\nimport pandas as pd\nimport unittest.mock as mock\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n# Public dataset: NYC Taxi Data (Yellow Taxi, Jan 2023)\n# Note: Real S3 access requires AWS credentials configured.\n# We wrap this in a try/except block for demonstration resilience.\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to read from {S3_URI}...\")\n    # 'anon': True is required for public buckets without credentials\n    # Adding timeouts to fail fast if network is down\n    df_s3 = pd.read_parquet(S3_URI, storage_options={\n        \"anon\": True,\n        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n    })\n    print(\"Success natively reading S3!\")\n    print(df_s3.head())\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    # Mock DataFrame\n    df_s3 = pd.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n        \"passenger_count\": [1.0, 1.0, 2.0],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3.head())\n\n\n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# Public dataset: BigQuery Public Data (US States)\n\nGCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {GCS_URI}...\")\n    df_gcs = pd.read_csv(GCS_URI)\n    print(\"Success natively reading GCS!\")\n    print(df_gcs.head())\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pd.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs.head())",
    "language": "python",
    "output": "Attempting to read from s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (AioSession._create_client() got an unexpected keyword argument 'connect_timeout'). Mocking success for demo.\n   VendorID tpep_pickup_datetime  passenger_count  trip_distance  total_amount\n0         1  2023-01-01 00:32:10              1.0           0.97           9.3\n1         2  2023-01-01 00:55:08              1.0           1.10          14.3\n2         1  2023-01-01 00:25:04              2.0           0.20          12.3\n\nAttempting to read from gs://cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS!\n         name post_abbr\n0     Alabama        AL\n1      Alaska        AK\n2     Arizona        AZ\n3    Arkansas        AR\n4  California        CA\n"
  },
  "load_cloud_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"s3fs\",\n#     \"gcsfs\",\n#     \"fsspec\",\n# ]\n# ///\nimport polars as pl\nimport unittest.mock as mock\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n# Using NYC Taxi Data (Parquet) - Public Bucket\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to scan from {S3_URI}...\")\n    # Polars scan_parquet is lazy and highly optimized for cloud\n    # storage_options={\"anon\": True} for public buckets\n    # Adding timeouts to fail fast\n    q = pl.scan_parquet(S3_URI, storage_options={\n        \"anon\": True,\n        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n    })\n    \n    # Collect first 5 rows\n    df_s3 = q.limit(5).collect()\n    print(\"Success natively reading S3!\")\n    print(df_s3)\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    df_s3 = pl.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3)\n\n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# using BigQuery public data (CSV)\n\nGCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {GCS_URI}...\")\n    df_gcs = pl.read_csv(GCS_URI)\n    print(\"Success natively reading GCS!\")\n    print(df_gcs.head())\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pl.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs)",
    "language": "python",
    "output": "Attempting to scan from s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (invalid value for 'anon': 'True' (expected str)). Mocking success for demo.\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 VendorID \u2506 tpep_pickup_datetime \u2506 trip_distance \u2506 total_amount \u2502\n\u2502 ---      \u2506 ---                  \u2506 ---           \u2506 ---          \u2502\n\u2502 i64      \u2506 str                  \u2506 f64           \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 2023-01-01 00:32:10  \u2506 0.97          \u2506 9.3          \u2502\n\u2502 2        \u2506 2023-01-01 00:55:08  \u2506 1.1           \u2506 14.3         \u2502\n\u2502 1        \u2506 2023-01-01 00:25:04  \u2506 0.2           \u2506 12.3         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAttempting to read from gs://cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS!\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name       \u2506 post_abbr \u2502\n\u2502 ---        \u2506 ---       \u2502\n\u2502 str        \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alabama    \u2506 AL        \u2502\n\u2502 Alaska     \u2506 AK        \u2502\n\u2502 Arizona    \u2506 AZ        \u2502\n\u2502 Arkansas   \u2506 AR        \u2502\n\u2502 California \u2506 CA        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "load_cloud_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\n\n# ---------------------------------------------------------\n# AWS S3 (Simple Storage Service)\n# ---------------------------------------------------------\n\nS3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n\ntry:\n    print(f\"Attempting to query {S3_URI}...\")\n    # DuckDB's httpfs extension handles S3/GCS\n    duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n    \n    # Configure for anonymous access usually works for public buckets\n    # duckdb.sql(\"SET s3_region='us-east-1';\")\n    \n    # Query directly\n    df_s3 = duckdb.sql(f\"SELECT * FROM read_parquet('{S3_URI}') LIMIT 5\").df()\n    print(\"Success natively reading S3!\")\n    print(df_s3)\nexcept Exception as e:\n    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n    \n    # Mock output\n    df_s3 = pd.DataFrame({\n        \"VendorID\": [1, 2, 1],\n        \"trip_distance\": [0.97, 1.10, 0.20],\n        \"total_amount\": [9.30, 14.30, 12.30]\n    })\n    print(df_s3)\n    \n# ---------------------------------------------------------\n# Google Cloud Storage (GCS)\n# ---------------------------------------------------------\n# DuckDB reads GCS via S3 compatibility layer or HTTPS for public files.\n# For public files, HTTPS is robust.\nHTTPS_URI = \"https://storage.googleapis.com/cloud-samples-data/bigquery/us-states/us-states.csv\"\n\ntry:\n    print(f\"\\nAttempting to read from {HTTPS_URI}...\")\n    df_gcs = duckdb.sql(f\"SELECT * FROM read_csv_auto('{HTTPS_URI}') LIMIT 5\").df()\n    print(\"Success natively reading GCS (via HTTPS)!\")\n    print(df_gcs)\nexcept Exception as e:\n    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n    \n    df_gcs = pd.DataFrame({\n        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n    })\n    print(df_gcs)",
    "language": "python",
    "output": "Attempting to query s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet...\nWarning: S3 Access failed (HTTP Error: HTTP GET error on 'https://nyc-tlc.s3.amazonaws.com/trip%20data/yellow_tripdata_2023-01.parquet' (HTTP 403)\n\nAuthentication Failure - this is usually caused by invalid or missing credentials.\n* No credentials are provided.\n* See https://duckdb.org/docs/stable/extensions/httpfs/s3api.html). Mocking success for demo.\n   VendorID  trip_distance  total_amount\n0         1           0.97           9.3\n1         2           1.10          14.3\n2         1           0.20          12.3\n\nAttempting to read from https://storage.googleapis.com/cloud-samples-data/bigquery/us-states/us-states.csv...\nSuccess natively reading GCS (via HTTPS)!\n         name post_abbr\n0     Alabama        AL\n1      Alaska        AK\n2     Arizona        AZ\n3    Arkansas        AR\n4  California        CA\n"
  },
  "transform_filter_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    # Mock data for demonstration\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (Boolean Indexing)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\nadelie_df = df[df['species'] == 'Adelie']\nprint(\"\\n--- Filter: Species == 'Adelie' ---\")\nprint(adelie_df.head(3))\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\n# Note: Parentheses are mandatory for multiple conditions in Pandas!\ncondition = (df['species'] == 'Adelie') & (df['island'] == 'Torgersen')\ntorgersen_adelie = df[condition]\nprint(\"\\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\")\nprint(torgersen_adelie.head(3))\n\n# ---------------------------------------------------------\n# 3. Using .query() (String Syntax)\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nlong_bills = df.query(\"bill_length_mm > 45\")\nprint(\"\\n--- Filter: bill_length_mm > 45 (using .query()) ---\")\nprint(long_bills.head(3))\n\n# ---------------------------------------------------------\n# 4. String Matching\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\nbiscoe_df = df[df['island'].str.startswith('B')]\nprint(\"\\n--- Filter: Island starts with 'B' ---\")\nprint(biscoe_df.head(3))\n\n# ---------------------------------------------------------\n# 5. Null Handling\n# ---------------------------------------------------------\n# Drop rows where 'sex' is missing\nclean_df = df.dropna(subset=['sex'])\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nprint(f\"Original: {len(df)}, Cleaned: {len(clean_df)}\")",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Filter: Species == 'Adelie' ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Filter: bill_length_mm > 45 (using .query()) ---\n    species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g   sex\n19   Adelie  Torgersen            46.0  ...              194.0       4200.0  MALE\n73   Adelie  Torgersen            45.8  ...              197.0       4150.0  MALE\n111  Adelie     Biscoe            45.6  ...              191.0       4600.0  MALE\n\n[3 rows x 7 columns]\n\n--- Filter: Island starts with 'B' ---\n   species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n20  Adelie  Biscoe            37.8  ...              174.0       3400.0  FEMALE\n21  Adelie  Biscoe            37.7  ...              180.0       3600.0    MALE\n22  Adelie  Biscoe            35.9  ...              189.0       3800.0  FEMALE\n\n[3 rows x 7 columns]\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344, Cleaned: 333\n"
  },
  "transform_filter_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"fsspec\",\n#     \"requests\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    # Polars can read from URL directly\n    df = pl.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (pl.col expression)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\n# In Polars, .filter() is the primary method\nadelie_df = df.filter(pl.col(\"species\") == \"Adelie\")\nprint(\"\\n--- Filter: Species == 'Adelie' ---\")\nprint(adelie_df.head(3))\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\n# Use & for AND, | for OR. Parentheses are required!\ntorgersen_adelie = df.filter(\n    (pl.col(\"species\") == \"Adelie\") & \n    (pl.col(\"island\") == \"Torgersen\")\n)\nprint(\"\\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\")\nprint(torgersen_adelie.head(3))\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nlong_bills = df.filter(pl.col(\"bill_length_mm\") > 45)\nprint(\"\\n--- Filter: bill_length_mm > 45 ---\")\nprint(long_bills.head(3))\n\n# ---------------------------------------------------------\n# 4. String Matching\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\nbiscoe_df = df.filter(pl.col(\"island\").str.starts_with(\"B\"))\nprint(\"\\n--- Filter: Island starts with 'B' ---\")\nprint(biscoe_df.head(3))\n\n# ---------------------------------------------------------\n# 5. Null Handling\n# ---------------------------------------------------------\n# Drop rows where 'sex' is null\nclean_df = df.drop_nulls(subset=[\"sex\"])\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nprint(f\"Original: {len(df)}, Cleaned: {len(clean_df)}\")",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Filter: Species == 'Adelie' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: Species == 'Adelie' AND Island == 'Torgersen' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 39.1           \u2506 18.7          \u2506 181               \u2506 3750        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 39.5           \u2506 17.4          \u2506 186               \u2506 3800        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 40.3           \u2506 18.0          \u2506 195               \u2506 3250        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: bill_length_mm > 45 ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island    \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex  \u2502\n\u2502 ---     \u2506 ---       \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---  \u2502\n\u2502 str     \u2506 str       \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Torgersen \u2506 46.0           \u2506 21.5          \u2506 194               \u2506 4200        \u2506 MALE \u2502\n\u2502 Adelie  \u2506 Torgersen \u2506 45.8           \u2506 18.9          \u2506 197               \u2506 4150        \u2506 MALE \u2502\n\u2502 Adelie  \u2506 Biscoe    \u2506 45.6           \u2506 20.3          \u2506 191               \u2506 4600        \u2506 MALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Filter: Island starts with 'B' ---\nshape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 island \u2506 bill_length_mm \u2506 bill_depth_mm \u2506 flipper_length_mm \u2506 body_mass_g \u2506 sex    \u2502\n\u2502 ---     \u2506 ---    \u2506 ---            \u2506 ---           \u2506 ---               \u2506 ---         \u2506 ---    \u2502\n\u2502 str     \u2506 str    \u2506 f64            \u2506 f64           \u2506 i64               \u2506 i64         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 Biscoe \u2506 37.8           \u2506 18.3          \u2506 174               \u2506 3400        \u2506 FEMALE \u2502\n\u2502 Adelie  \u2506 Biscoe \u2506 37.7           \u2506 18.7          \u2506 180               \u2506 3600        \u2506 MALE   \u2502\n\u2502 Adelie  \u2506 Biscoe \u2506 35.9           \u2506 19.2          \u2506 189               \u2506 3800        \u2506 FEMALE \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344, Cleaned: 333\n"
  },
  "transform_filter_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nTABLE_NAME = \"penguins\"\n\ntry:\n    # DuckDB handles HTTPS if httpfs is installed/loaded (auto-loaded in modern versions)\n    # We create a view or table to query against easily\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM '{URL}'\")\n    print(f\"Loaded data from {URL} into DuckDB table '{TABLE_NAME}'\")\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data via Pandas.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    # Create a Pandas DataFrame\n    pandas_df = pd.read_csv(io.StringIO(data))\n    # DuckDB can query Pandas DataFrames directly!\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM pandas_df\")\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (SQL WHERE)\n# ---------------------------------------------------------\n# Select only 'Adelie' penguins\nprint(\"\\n--- Filter: Species = 'Adelie' ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE species = 'Adelie' LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions (AND/OR)\n# ---------------------------------------------------------\n# Select 'Adelie' penguins from 'Torgersen' island\nprint(\"\\n--- Filter: Species = 'Adelie' AND Island = 'Torgersen' ---\")\nduckdb.sql(f\"\"\"\n    SELECT * FROM {TABLE_NAME} \n    WHERE species = 'Adelie' AND island = 'Torgersen' \n    LIMIT 3\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\n# Select penguins with bill length > 45mm\nprint(\"\\n--- Filter: bill_length_mm > 45 ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE bill_length_mm > 45 LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 4. String Matching (LIKE/ILIKE)\n# ---------------------------------------------------------\n# Select penguins where island starts with 'B' (Biscoe)\n# '%' is the wildcard in SQL\nprint(\"\\n--- Filter: Island LIKE 'B%' ---\")\nduckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE island LIKE 'B%' LIMIT 3\").show()\n\n# ---------------------------------------------------------\n# 5. Null Handling (IS NOT NULL)\n# ---------------------------------------------------------\n# Drop rows where 'sex' is null\nprint(f\"\\n--- Drop Nulls in 'sex' ---\")\nresult = duckdb.sql(f\"SELECT * FROM {TABLE_NAME} WHERE sex IS NOT NULL\")\nprint(f\"Original: {duckdb.sql(f'SELECT count(*) FROM {TABLE_NAME}').fetchone()[0]}\")\nprint(f\"Cleaned: {result.count('*').fetchone()[0]}\")",
    "language": "python",
    "output": "Loaded data from https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv into DuckDB table 'penguins'\n\n--- Filter: Species = 'Adelie' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: Species = 'Adelie' AND Island = 'Torgersen' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           39.1 \u2502          18.7 \u2502               181 \u2502        3750 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           39.5 \u2502          17.4 \u2502               186 \u2502        3800 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           40.3 \u2502          18.0 \u2502               195 \u2502        3250 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: bill_length_mm > 45 ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502  island   \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502  varchar  \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Torgersen \u2502           46.0 \u2502          21.5 \u2502               194 \u2502        4200 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Torgersen \u2502           45.8 \u2502          18.9 \u2502               197 \u2502        4150 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Biscoe    \u2502           45.6 \u2502          20.3 \u2502               191 \u2502        4600 \u2502 MALE    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Filter: Island LIKE 'B%' ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502 island  \u2502 bill_length_mm \u2502 bill_depth_mm \u2502 flipper_length_mm \u2502 body_mass_g \u2502   sex   \u2502\n\u2502 varchar \u2502 varchar \u2502     double     \u2502    double     \u2502       int64       \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 Biscoe  \u2502           37.8 \u2502          18.3 \u2502               174 \u2502        3400 \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 Biscoe  \u2502           37.7 \u2502          18.7 \u2502               180 \u2502        3600 \u2502 MALE    \u2502\n\u2502 Adelie  \u2502 Biscoe  \u2502           35.9 \u2502          19.2 \u2502               189 \u2502        3800 \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Drop Nulls in 'sex' ---\nOriginal: 344\nCleaned: 333\n"
  },
  "transform_filter_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"db-dtypes\",\n# ]\n# ///\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Mock Setup (Simulating BigQuery)\n# ---------------------------------------------------------\n# In a real scenario, you would use:\n# client = bigquery.Client()\n\nmock_client = mock.MagicMock(spec=bigquery.Client)\nprint(\"--- BigQuery Client Initialized (Mock) ---\\n\")\n\n# Load mock data for results\ndata = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\n\"\"\"\ndf_mock = pd.read_csv(io.StringIO(data))\n\ndef mock_query(query):\n    print(f\"Executing SQL:\\n{query}\\n\")\n    \n    # We'll just filter the mock dataframe to simulate the query result\n    # ensuring the output matches the SQL intent roughly\n    \n    result = df_mock.copy()\n    \n    if \"species = 'Adelie'\" in query and \"island = 'Torgersen'\" in query:\n        result = result[(result['species'] == 'Adelie') & (result['island'] == 'Torgersen')]\n    elif \"species = 'Adelie'\" in query:\n        result = result[result['species'] == 'Adelie']\n    elif \"bill_length_mm > 45\" in query:\n        result = result[result['bill_length_mm'] > 45]\n    elif \"island LIKE 'B%'\" in query:\n        result = result[result['island'].str.startswith('B')]\n    \n    # Mock row iterator\n    mock_job = mock.MagicMock()\n    mock_job.to_dataframe.return_value = result.head(3)\n    return mock_job\n\nmock_client.query.side_effect = mock_query\n\n# ---------------------------------------------------------\n# 1. Basic Filtering (SQL WHERE)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 2. Multiple Conditions (AND/OR)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie' AND island = 'Torgersen'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 3. Numeric Comparison\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE bill_length_mm > 45\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 4. String Matching (LIKE)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE island LIKE 'B%'\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie'\n    LIMIT 3\n\n\n--- Result ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...                181         3750    Male\n1  Adelie  Torgersen            39.5  ...                186         3800  Female\n2  Adelie  Torgersen            40.3  ...                195         3250  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE species = 'Adelie' AND island = 'Torgersen'\n    LIMIT 3\n\n\n\n--- Result ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...                181         3750    Male\n1  Adelie  Torgersen            39.5  ...                186         3800  Female\n2  Adelie  Torgersen            40.3  ...                195         3250  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE bill_length_mm > 45\n    LIMIT 3\n\n\n\n--- Result ---\n     species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n5  Chinstrap   Dream            46.5  ...                192         3500  Female\n6     Gentoo  Biscoe            46.1  ...                211         4500  Female\n\n[2 rows x 7 columns]\nExecuting SQL:\n\n    SELECT * \n    FROM `my-project.dataset.penguins`\n    WHERE island LIKE 'B%'\n    LIMIT 3\n\n\n\n--- Result ---\n  species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n6  Gentoo  Biscoe            46.1  ...                211         4500  Female\n\n[1 rows x 7 columns]\n"
  },
  "transform_sort_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Sorting (Single Column)\n# ---------------------------------------------------------\n# Sort by bill length (descending) - longest bills first\nsorted_df = df.sort_values(by=\"bill_length_mm\", ascending=False)\nprint(\"\\n--- Sort: bill_length_mm (Descending) ---\")\nprint(sorted_df[[\"species\", \"bill_length_mm\"]].head(3))\n\n# ---------------------------------------------------------\n# 2. Multi-Column Sorting\n# ---------------------------------------------------------\n# Sort by Island (A-Z), then by Body Mass (Heaviest first)\nmulti_sort = df.sort_values(\n    by=[\"island\", \"body_mass_g\"], \n    ascending=[True, False]\n)\nprint(\"\\n--- Sort: Island (ASC) then Body Mass (DESC) ---\")\nprint(multi_sort[[\"island\", \"body_mass_g\", \"species\"]].head(3))\n\n# ---------------------------------------------------------\n# 3. Top N Results (nlargest)\n# ---------------------------------------------------------\n# Get the 3 heaviest penguins\nheaviest = df.nlargest(3, \"body_mass_g\")\nprint(\"\\n--- Top 3 Heaviest Penguins ---\")\nprint(heaviest[[\"species\", \"body_mass_g\"]])\n\n# ---------------------------------------------------------\n# 4. Sorting by Index\n# ---------------------------------------------------------\n# Sometimes you want to return to original order or sort by index\nsorted_by_index = df.sort_index()\nprint(\"\\n--- Sort by Index ---\")\nprint(sorted_by_index.head(3))",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Sort: bill_length_mm (Descending) ---\n       species  bill_length_mm\n253     Gentoo            59.6\n169  Chinstrap            58.0\n321     Gentoo            55.9\n\n--- Sort: Island (ASC) then Body Mass (DESC) ---\n     island  body_mass_g species\n237  Biscoe       6300.0  Gentoo\n253  Biscoe       6050.0  Gentoo\n297  Biscoe       6000.0  Gentoo\n\n--- Top 3 Heaviest Penguins ---\n    species  body_mass_g\n237  Gentoo       6300.0\n253  Gentoo       6050.0\n297  Gentoo       6000.0\n\n--- Sort by Index ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  FEMALE\n\n[3 rows x 7 columns]\n"
  },
  "transform_sort_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"fsspec\",\n#     \"requests\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pl.read_csv(URL)\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Sorting (Single Column)\n# ---------------------------------------------------------\n# Sort by bill length (descending)\n# In Polars, use .sort() with `descending=True`\nsorted_df = df.sort(\"bill_length_mm\", descending=True)\nprint(\"\\n--- Sort: bill_length_mm (Descending) ---\")\nprint(sorted_df[[\"species\", \"bill_length_mm\"]].head(3))\n\n# ---------------------------------------------------------\n# 2. Multi-Column Sorting\n# ---------------------------------------------------------\n# Sort by Island (A-Z), then by Body Mass (Heaviest first)\n# Pass lists to both arguments\nmulti_sort = df.sort(\n    [\"island\", \"body_mass_g\"], \n    descending=[False, True]\n)\nprint(\"\\n--- Sort: Island (ASC) then Body Mass (DESC) ---\")\nprint(multi_sort[[\"island\", \"body_mass_g\", \"species\"]].head(3))\n\n# ---------------------------------------------------------\n# 3. Top N Results (top_k efficient sort)\n# ---------------------------------------------------------\n# Get the 3 heaviest penguins efficiently\n# The dataset might have nulls, top_k handles them\nheaviest = df.top_k(3, by=\"body_mass_g\")\nprint(\"\\n--- Top 3 Heaviest Penguins (top_k) ---\")\nprint(heaviest[[\"species\", \"body_mass_g\"]])\n\n# ---------------------------------------------------------\n# 4. Null Handling\n# ---------------------------------------------------------\n# You can specify nulls_last=True explicitly if needed\n# sorted_df = df.sort(\"bill_length_mm\", descending=True, nulls_last=True)",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Sort: bill_length_mm (Descending) ---\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 bill_length_mm \u2502\n\u2502 ---     \u2506 ---            \u2502\n\u2502 str     \u2506 f64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 null           \u2502\n\u2502 Gentoo  \u2506 null           \u2502\n\u2502 Gentoo  \u2506 59.6           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Sort: Island (ASC) then Body Mass (DESC) ---\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 island \u2506 body_mass_g \u2506 species \u2502\n\u2502 ---    \u2506 ---         \u2506 ---     \u2502\n\u2502 str    \u2506 i64         \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Biscoe \u2506 null        \u2506 Gentoo  \u2502\n\u2502 Biscoe \u2506 6300        \u2506 Gentoo  \u2502\n\u2502 Biscoe \u2506 6050        \u2506 Gentoo  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Top 3 Heaviest Penguins (top_k) ---\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 body_mass_g \u2502\n\u2502 ---     \u2506 ---         \u2502\n\u2502 str     \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Gentoo  \u2506 6300        \u2502\n\u2502 Gentoo  \u2506 6050        \u2502\n\u2502 Gentoo  \u2506 6000        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "transform_sort_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\nTABLE_NAME = \"penguins\"\n\ntry:\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM '{URL}'\")\n    print(f\"Loaded data from {URL} into DuckDB table '{TABLE_NAME}'\")\nexcept Exception:\n    print(\"Warning: Failed to load data from URL. Mocking data via Pandas.\")\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nChinstrap,Dream,50.0,19.5,196,3900,Male\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    pandas_df = pd.read_csv(io.StringIO(data))\n    duckdb.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM pandas_df\")\n\n# ---------------------------------------------------------\n# 1. Basic Sorting (ORDER BY)\n# ---------------------------------------------------------\n# Sort by bill length (descending)\nprint(\"\\n--- Sort: bill_length_mm DESC ---\")\nduckdb.sql(f\"\"\"\n    SELECT species, bill_length_mm \n    FROM {TABLE_NAME} \n    ORDER BY bill_length_mm DESC \n    LIMIT 3\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 2. Multi-Column Sorting\n# ---------------------------------------------------------\n# Sort by Island (ASC), then by Body Mass (DESC)\nprint(\"\\n--- Sort: Island ASC, Body Mass DESC ---\")\nduckdb.sql(f\"\"\"\n    SELECT island, body_mass_g, species \n    FROM {TABLE_NAME} \n    ORDER BY island ASC, body_mass_g DESC \n    LIMIT 3\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 3. Handling Nulls (NULLS FIRST/LAST)\n# ---------------------------------------------------------\n# Sort by sex, putting NULLs at the end\nprint(\"\\n--- Sort: Sex ASC NULLS LAST ---\")\nduckdb.sql(f\"\"\"\n    SELECT species, sex \n    FROM {TABLE_NAME} \n    ORDER BY sex ASC NULLS LAST \n    LIMIT 3\n\"\"\").show()",
    "language": "python",
    "output": "Loaded data from https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv into DuckDB table 'penguins'\n\n--- Sort: bill_length_mm DESC ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  species  \u2502 bill_length_mm \u2502\n\u2502  varchar  \u2502     double     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Gentoo    \u2502           59.6 \u2502\n\u2502 Chinstrap \u2502           58.0 \u2502\n\u2502 Gentoo    \u2502           55.9 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Sort: Island ASC, Body Mass DESC ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 island  \u2502 body_mass_g \u2502 species \u2502\n\u2502 varchar \u2502    int64    \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Biscoe  \u2502        6300 \u2502 Gentoo  \u2502\n\u2502 Biscoe  \u2502        6050 \u2502 Gentoo  \u2502\n\u2502 Biscoe  \u2502        6000 \u2502 Gentoo  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Sort: Sex ASC NULLS LAST ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2502   sex   \u2502\n\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Adelie  \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 FEMALE  \u2502\n\u2502 Adelie  \u2502 FEMALE  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "transform_sort_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"db-dtypes\",\n# ]\n# ///\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Mock Setup (Simulating BigQuery)\n# ---------------------------------------------------------\n# In a real scenario, you would use:\n# client = bigquery.Client()\n\nmock_client = mock.MagicMock(spec=bigquery.Client)\nprint(\"--- BigQuery Client Initialized (Mock) ---\\n\")\n\n# Load mock data for results\ndata = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Torgersen,40.3,18.0,195,3250,Female\nAdelie,Torgersen,36.7,19.3,193,3450,Female\nAdelie,Torgersen,39.3,20.6,190,3650,Male\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\n\"\"\"\ndf_mock = pd.read_csv(io.StringIO(data))\n\ndef mock_query(query):\n    print(f\"Executing SQL:\\n{query}\\n\")\n    \n    # We'll just sort the mock dataframe to simulate the query result\n    \n    result = df_mock.copy()\n    \n    if \"ORDER BY bill_length_mm DESC\" in query:\n        result = result.sort_values(\"bill_length_mm\", ascending=False)\n    elif \"ORDER BY island ASC, body_mass_g DESC\" in query:\n        result = result.sort_values([\"island\", \"body_mass_g\"], ascending=[True, False])\n    elif \"ORDER BY sex ASC NULLS LAST\" in query:\n        # Pandas handles NaNs at end by default\n        result = result.sort_values(\"sex\", ascending=True, na_position='last')\n    \n    # Mock row iterator\n    mock_job = mock.MagicMock()\n    mock_job.to_dataframe.return_value = result.head(3)\n    return mock_job\n\nmock_client.query.side_effect = mock_query\n\n# ---------------------------------------------------------\n# 1. Basic Sorting (ORDER BY)\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT species, bill_length_mm \n    FROM `my-project.dataset.penguins`\n    ORDER BY bill_length_mm DESC\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 2. Multi-Column Sorting\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT island, body_mass_g, species \n    FROM `my-project.dataset.penguins`\n    ORDER BY island ASC, body_mass_g DESC\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 3. Handling Nulls\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT species, sex \n    FROM `my-project.dataset.penguins`\n    ORDER BY sex ASC NULLS LAST\n    LIMIT 3\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result ---\")\nprint(df)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\n    SELECT species, bill_length_mm \n    FROM `my-project.dataset.penguins`\n    ORDER BY bill_length_mm DESC\n    LIMIT 3\n\n\n--- Result ---\n     species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n5  Chinstrap      Dream            46.5  ...                192         3500  Female\n6     Gentoo     Biscoe            46.1  ...                211         4500  Female\n2     Adelie  Torgersen            40.3  ...                195         3250  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT island, body_mass_g, species \n    FROM `my-project.dataset.penguins`\n    ORDER BY island ASC, body_mass_g DESC\n    LIMIT 3\n\n\n\n--- Result ---\n     species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n6     Gentoo     Biscoe            46.1  ...                211         4500  Female\n5  Chinstrap      Dream            46.5  ...                192         3500  Female\n1     Adelie  Torgersen            39.5  ...                186         3800  Female\n\n[3 rows x 7 columns]\nExecuting SQL:\n\n    SELECT species, sex \n    FROM `my-project.dataset.penguins`\n    ORDER BY sex ASC NULLS LAST\n    LIMIT 3\n\n\n\n--- Result ---\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n1  Adelie  Torgersen            39.5  ...                186         3800  Female\n2  Adelie  Torgersen            40.3  ...                195         3250  Female\n3  Adelie  Torgersen            36.7  ...                193         3450  Female\n\n[3 rows x 7 columns]\n"
  },
  "transform_join_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\n\n# ---------------------------------------------------------\n# Define Datasets\n# ---------------------------------------------------------\nemployees = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept_id': [10, 10, 20, 99]  # David has a non-existent dept\n})\n\ndepartments = pd.DataFrame({\n    'dept_id': [10, 20, 30],\n    'dept_name': ['HR', 'Engineering', 'Marketing'] # Marketing has no employees\n})\n\nprint(\"--- Employees ---\")\nprint(employees)\nprint(\"\\n--- Departments ---\")\nprint(departments)\n\n# ---------------------------------------------------------\n# 1. Inner Join (Matched records only)\n# ---------------------------------------------------------\ninner_join = pd.merge(employees, departments, on='dept_id', how='inner')\nprint(\"\\n--- Inner Join (how='inner') ---\")\nprint(inner_join)\n\n# ---------------------------------------------------------\n# 2. Left Join (All employees, even without a department match)\n# ---------------------------------------------------------\nleft_join = pd.merge(employees, departments, on='dept_id', how='left')\nprint(\"\\n--- Left Join (how='left') ---\")\nprint(left_join)\n\n# ---------------------------------------------------------\n# 3. Outer Join (All employees and all departments)\n# ---------------------------------------------------------\nouter_join = pd.merge(employees, departments, on='dept_id', how='outer')\nprint(\"\\n--- Outer Join (how='outer') ---\")\nprint(outer_join)\n\n# ---------------------------------------------------------\n# 4. Right Join (All departments, even if no employees belong to them)\n# ---------------------------------------------------------\nright_join = pd.merge(employees, departments, on='dept_id', how='right')\nprint(\"\\n--- Right Join (how='right') ---\")\nprint(right_join)",
    "language": "python",
    "output": "--- Employees ---\n   id     name  dept_id\n0   1    Alice       10\n1   2      Bob       10\n2   3  Charlie       20\n3   4    David       99\n\n--- Departments ---\n   dept_id    dept_name\n0       10           HR\n1       20  Engineering\n2       30    Marketing\n\n--- Inner Join (how='inner') ---\n   id     name  dept_id    dept_name\n0   1    Alice       10           HR\n1   2      Bob       10           HR\n2   3  Charlie       20  Engineering\n\n--- Left Join (how='left') ---\n   id     name  dept_id    dept_name\n0   1    Alice       10           HR\n1   2      Bob       10           HR\n2   3  Charlie       20  Engineering\n3   4    David       99          NaN\n\n--- Outer Join (how='outer') ---\n    id     name  dept_id    dept_name\n0  1.0    Alice       10           HR\n1  2.0      Bob       10           HR\n2  3.0  Charlie       20  Engineering\n3  NaN      NaN       30    Marketing\n4  4.0    David       99          NaN\n\n--- Right Join (how='right') ---\n    id     name  dept_id    dept_name\n0  1.0    Alice       10           HR\n1  2.0      Bob       10           HR\n2  3.0  Charlie       20  Engineering\n3  NaN      NaN       30    Marketing\n"
  },
  "transform_join_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\nimport polars as pl\n\n# ---------------------------------------------------------\n# Define Datasets\n# ---------------------------------------------------------\nemployees = pl.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept_id': [10, 10, 20, 99]\n})\n\ndepartments = pl.DataFrame({\n    'dept_id': [10, 20, 30],\n    'dept_name': ['HR', 'Engineering', 'Marketing']\n})\n\nprint(\"--- Employees ---\")\nprint(employees)\nprint(\"\\n--- Departments ---\")\nprint(departments)\n\n# ---------------------------------------------------------\n# 1. Inner Join\n# ---------------------------------------------------------\ninner_join = employees.join(departments, on='dept_id', how='inner')\nprint(\"\\n--- Inner Join (how='inner') ---\")\nprint(inner_join)\n\n# ---------------------------------------------------------\n# 2. Left Join\n# ---------------------------------------------------------\nleft_join = employees.join(departments, on='dept_id', how='left')\nprint(\"\\n--- Left Join (how='left') ---\")\nprint(left_join)\n\n# ---------------------------------------------------------\n# 3. Full Outer Join\n# ---------------------------------------------------------\n# Note: Polars uses how='full'\nouter_join = employees.join(departments, on='dept_id', how='full')\nprint(\"\\n--- Full Outer Join (how='full') ---\")\nprint(outer_join)\n\n# ---------------------------------------------------------\n# 4. Anti Join (Polars Special)\n# ---------------------------------------------------------\n# Find employees who are NOT in any department (or an invalid one)\nanti_join = employees.join(departments, on='dept_id', how='anti')\nprint(\"\\n--- Anti Join (Employees without a valid Dept) ---\")\nprint(anti_join)",
    "language": "python",
    "output": "--- Employees ---\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept_id \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2502\n\u2502 i64 \u2506 str     \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 10      \u2502\n\u2502 2   \u2506 Bob     \u2506 10      \u2502\n\u2502 3   \u2506 Charlie \u2506 20      \u2502\n\u2502 4   \u2506 David   \u2506 99      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Departments ---\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept_id \u2506 dept_name   \u2502\n\u2502 ---     \u2506 ---         \u2502\n\u2502 i64     \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 10      \u2506 HR          \u2502\n\u2502 20      \u2506 Engineering \u2502\n\u2502 30      \u2506 Marketing   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Inner Join (how='inner') ---\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept_id \u2506 dept_name   \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2506 ---         \u2502\n\u2502 i64 \u2506 str     \u2506 i64     \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 10      \u2506 HR          \u2502\n\u2502 2   \u2506 Bob     \u2506 10      \u2506 HR          \u2502\n\u2502 3   \u2506 Charlie \u2506 20      \u2506 Engineering \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Left Join (how='left') ---\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name    \u2506 dept_id \u2506 dept_name   \u2502\n\u2502 --- \u2506 ---     \u2506 ---     \u2506 ---         \u2502\n\u2502 i64 \u2506 str     \u2506 i64     \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Alice   \u2506 10      \u2506 HR          \u2502\n\u2502 2   \u2506 Bob     \u2506 10      \u2506 HR          \u2502\n\u2502 3   \u2506 Charlie \u2506 20      \u2506 Engineering \u2502\n\u2502 4   \u2506 David   \u2506 99      \u2506 null        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Full Outer Join (how='full') ---\nshape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id   \u2506 name    \u2506 dept_id \u2506 dept_id_right \u2506 dept_name   \u2502\n\u2502 ---  \u2506 ---     \u2506 ---     \u2506 ---           \u2506 ---         \u2502\n\u2502 i64  \u2506 str     \u2506 i64     \u2506 i64           \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 Alice   \u2506 10      \u2506 10            \u2506 HR          \u2502\n\u2502 2    \u2506 Bob     \u2506 10      \u2506 10            \u2506 HR          \u2502\n\u2502 3    \u2506 Charlie \u2506 20      \u2506 20            \u2506 Engineering \u2502\n\u2502 4    \u2506 David   \u2506 99      \u2506 null          \u2506 null        \u2502\n\u2502 null \u2506 null    \u2506 null    \u2506 30            \u2506 Marketing   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Anti Join (Employees without a valid Dept) ---\nshape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name  \u2506 dept_id \u2502\n\u2502 --- \u2506 ---   \u2506 ---     \u2502\n\u2502 i64 \u2506 str   \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4   \u2506 David \u2506 99      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "transform_join_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\n\n# ---------------------------------------------------------\n# Define Datasets in Pandas for easy loading into DuckDB\n# ---------------------------------------------------------\nemployees_df = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept_id': [10, 10, 20, 99]\n})\n\ndepartments_df = pd.DataFrame({\n    'dept_id': [10, 20, 30],\n    'dept_name': ['HR', 'Engineering', 'Marketing']\n})\n\n# Create DuckDB tables\nduckdb.sql(\"CREATE TABLE employees AS SELECT * FROM employees_df\")\nduckdb.sql(\"CREATE TABLE departments AS SELECT * FROM departments_df\")\n\nprint(\"--- Employees ---\")\nduckdb.sql(\"SELECT * FROM employees\").show()\nprint(\"\\n--- Departments ---\")\nduckdb.sql(\"SELECT * FROM departments\").show()\n\n# ---------------------------------------------------------\n# 1. Inner Join\n# ---------------------------------------------------------\nprint(\"\\n--- Inner Join ---\")\nduckdb.sql(\"\"\"\n    SELECT e.name, d.dept_name\n    FROM employees e\n    INNER JOIN departments d ON e.dept_id = d.dept_id\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 2. Left Join\n# ---------------------------------------------------------\nprint(\"\\n--- Left Outer Join ---\")\nduckdb.sql(\"\"\"\n    SELECT e.name, d.dept_name\n    FROM employees e\n    LEFT JOIN departments d ON e.dept_id = d.dept_id\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 3. Full Join\n# ---------------------------------------------------------\nprint(\"\\n--- Full Outer Join ---\")\nduckdb.sql(\"\"\"\n    SELECT e.name, d.dept_name\n    FROM employees e\n    FULL JOIN departments d ON e.dept_id = d.dept_id\n\"\"\").show()\n\n# ---------------------------------------------------------\n# 4. Joining on Multiple Columns (Example)\n# ---------------------------------------------------------\n# duckdb.sql(\"... JOIN ... ON a.id = b.id AND a.year = b.year\")",
    "language": "python",
    "output": "--- Employees ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502  name   \u2502 dept_id \u2502\n\u2502 int64 \u2502 varchar \u2502  int64  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Alice   \u2502      10 \u2502\n\u2502     2 \u2502 Bob     \u2502      10 \u2502\n\u2502     3 \u2502 Charlie \u2502      20 \u2502\n\u2502     4 \u2502 David   \u2502      99 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Departments ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dept_id \u2502  dept_name  \u2502\n\u2502  int64  \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      10 \u2502 HR          \u2502\n\u2502      20 \u2502 Engineering \u2502\n\u2502      30 \u2502 Marketing   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Inner Join ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502  dept_name  \u2502\n\u2502 varchar \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice   \u2502 HR          \u2502\n\u2502 Bob     \u2502 HR          \u2502\n\u2502 Charlie \u2502 Engineering \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Left Outer Join ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502  dept_name  \u2502\n\u2502 varchar \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice   \u2502 HR          \u2502\n\u2502 Bob     \u2502 HR          \u2502\n\u2502 Charlie \u2502 Engineering \u2502\n\u2502 David   \u2502 NULL        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n--- Full Outer Join ---\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  name   \u2502  dept_name  \u2502\n\u2502 varchar \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice   \u2502 HR          \u2502\n\u2502 Bob     \u2502 HR          \u2502\n\u2502 Charlie \u2502 Engineering \u2502\n\u2502 David   \u2502 NULL        \u2502\n\u2502 NULL    \u2502 Marketing   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
  },
  "transform_join_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n#     \"db-dtypes\",\n# ]\n# ///\nimport unittest.mock as mock\nfrom google.cloud import bigquery\nimport pandas as pd\n\n# ---------------------------------------------------------\n# Mock Setup (Simulating BigQuery)\n# ---------------------------------------------------------\nmock_client = mock.MagicMock(spec=bigquery.Client)\nprint(\"--- BigQuery Client Initialized (Mock) ---\\n\")\n\n# Prepare mock datasets\nemployees = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'dept_id': [10, 10, 20, 99]\n})\n\ndepartments = pd.DataFrame({\n    'dept_id': [10, 20, 30],\n    'dept_name': ['HR', 'Engineering', 'Marketing']\n})\n\ndef mock_query(query):\n    query_clean = query.strip().upper()\n    print(f\"Executing SQL:\\n{query}\\n\")\n    \n    # Very simple simulation of SQL Join logic\n    if \"INNER JOIN\" in query_clean:\n        result = pd.merge(employees, departments, on='dept_id', how='inner')\n    elif \"LEFT JOIN\" in query_clean:\n        result = pd.merge(employees, departments, on='dept_id', how='left')\n    elif \"FULL JOIN\" in query_clean or \"FULL OUTER JOIN\" in query_clean:\n        result = pd.merge(employees, departments, on='dept_id', how='outer')\n    else:\n        result = employees.copy()\n    \n    # Return mock job with dataframe result\n    mock_job = mock.MagicMock()\n    mock_job.to_dataframe.return_value = result[['name', 'dept_name']] if 'dept_name' in result.columns else result\n    return mock_job\n\nmock_client.query.side_effect = mock_query\n\n# ---------------------------------------------------------\n# 1. Inner Join\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    INNER JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"--- Result: Inner Join ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 2. Left Join\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    LEFT JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result: Left Join ---\")\nprint(df)\n\n# ---------------------------------------------------------\n# 3. Full Join\n# ---------------------------------------------------------\nquery = \"\"\"\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    FULL OUTER JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\"\"\"\ndf = mock_client.query(query).to_dataframe()\nprint(\"\\n--- Result: Full Join ---\")\nprint(df)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    INNER JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\n\n--- Result: Inner Join ---\n      name    dept_name\n0    Alice           HR\n1      Bob           HR\n2  Charlie  Engineering\nExecuting SQL:\n\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    LEFT JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\n\n\n--- Result: Left Join ---\n      name    dept_name\n0    Alice           HR\n1      Bob           HR\n2  Charlie  Engineering\n3    David          NaN\nExecuting SQL:\n\n    SELECT e.name, d.dept_name\n    FROM `my-project.dataset.employees` e\n    FULL OUTER JOIN `my-project.dataset.departments` d ON e.dept_id = d.dept_id\n\n\n\n--- Result: Full Join ---\n      name    dept_name\n0    Alice           HR\n1      Bob           HR\n2  Charlie  Engineering\n3      NaN    Marketing\n4    David          NaN\n"
  },
  "transform_aggregate_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    # Mock data fallback\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Aggregation (Mean Body Mass by Species)\n# ---------------------------------------------------------\n# Simple groupby + mean\navg_mass = df.groupby('species')['body_mass_g'].mean().reset_index()\nprint(\"\\n--- Average Body Mass by Species ---\")\nprint(avg_mass)\n\n# ---------------------------------------------------------\n# 2. Multiple Aggregations (.agg)\n# ---------------------------------------------------------\n# Get count, mean, min, and max in one go\nstats = df.groupby('species')['body_mass_g'].agg(['count', 'mean', 'min', 'max'])\nprint(\"\\n--- Summary Statistics by Species ---\")\nprint(stats)\n\n# ---------------------------------------------------------\n# 3. Grouping by Multiple Columns\n# ---------------------------------------------------------\n# Average bill length by species and island\ngeo_stats = df.groupby(['species', 'island'])['bill_length_mm'].mean().reset_index()\nprint(\"\\n--- Average Bill Length (Species + Island) ---\")\nprint(geo_stats.head(5))\n\n# ---------------------------------------------------------\n# 4. Named Aggregation (Pandas 0.25+)\n# ---------------------------------------------------------\n# More descriptive column names in final result\nnamed_agg = df.groupby('species').agg(\n    total_penguins=('species', 'count'),\n    avg_weight_g=('body_mass_g', 'mean'),\n    max_bill_mm=('bill_length_mm', 'max')\n)\nprint(\"\\n--- Named Aggregations ---\")\nprint(named_agg)",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Average Body Mass by Species ---\n     species  body_mass_g\n0     Adelie  3700.662252\n1  Chinstrap  3733.088235\n2     Gentoo  5076.016260\n\n--- Summary Statistics by Species ---\n           count         mean     min     max\nspecies                                      \nAdelie       151  3700.662252  2850.0  4775.0\nChinstrap     68  3733.088235  2700.0  4800.0\nGentoo       123  5076.016260  3950.0  6300.0\n\n--- Average Bill Length (Species + Island) ---\n     species     island  bill_length_mm\n0     Adelie     Biscoe       38.975000\n1     Adelie      Dream       38.501786\n2     Adelie  Torgersen       38.950980\n3  Chinstrap      Dream       48.833824\n4     Gentoo     Biscoe       47.504878\n\n--- Named Aggregations ---\n           total_penguins  avg_weight_g  max_bill_mm\nspecies                                             \nAdelie                152   3700.662252         46.0\nChinstrap              68   3733.088235         58.0\nGentoo                124   5076.016260         59.6\n"
  },
  "transform_aggregate_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pl.read_csv(URL)\nexcept Exception:\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Basic Aggregation (Mean Mass by Species)\n# ---------------------------------------------------------\n# In Polars, group_by + agg is the standard flow\navg_mass = df.group_by(\"species\").agg(\n    pl.col(\"body_mass_g\").mean().alias(\"avg_mass\")\n)\nprint(\"\\n--- Average Body Mass by Species ---\")\nprint(avg_mass)\n\n# ---------------------------------------------------------\n# 2. Multiple Aggregations\n# ---------------------------------------------------------\n# Polars expressions allow many aggregates in one list\nstats = df.group_by(\"species\").agg([\n    pl.len().alias(\"count\"),\n    pl.col(\"body_mass_g\").mean().alias(\"mean\"),\n    pl.col(\"body_mass_g\").min().alias(\"min\"),\n    pl.col(\"body_mass_g\").max().alias(\"max\"),\n])\nprint(\"\\n--- Summary Statistics by Species ---\")\nprint(stats)\n\n# ---------------------------------------------------------\n# 3. Grouping by Multiple Columns\n# ---------------------------------------------------------\ngeo_stats = df.group_by([\"species\", \"island\"]).agg(\n    pl.col(\"bill_length_mm\").mean().alias(\"avg_bill\")\n).sort(\"avg_bill\", descending=True)\n\nprint(\"\\n--- Average Bill Length (Species + Island) ---\")\nprint(geo_stats.head(5))\n\n# ---------------------------------------------------------\n# 4. Expressions within Aggregation\n# ---------------------------------------------------------\n# Calculate the spread (max - min) while grouping\ncomplex_agg = df.group_by(\"species\").agg(\n    spread=(pl.col(\"body_mass_g\").max() - pl.col(\"body_mass_g\").min())\n)\nprint(\"\\n--- Body Mass Spread by Species ---\")\nprint(complex_agg)",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Average Body Mass by Species ---\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 avg_mass    \u2502\n\u2502 ---       \u2506 ---         \u2502\n\u2502 str       \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Gentoo    \u2506 5076.01626  \u2502\n\u2502 Adelie    \u2506 3700.662252 \u2502\n\u2502 Chinstrap \u2506 3733.088235 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Summary Statistics by Species ---\nshape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 count \u2506 mean        \u2506 min  \u2506 max  \u2502\n\u2502 ---       \u2506 ---   \u2506 ---         \u2506 ---  \u2506 ---  \u2502\n\u2502 str       \u2506 u32   \u2506 f64         \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Gentoo    \u2506 124   \u2506 5076.01626  \u2506 3950 \u2506 6300 \u2502\n\u2502 Chinstrap \u2506 68    \u2506 3733.088235 \u2506 2700 \u2506 4800 \u2502\n\u2502 Adelie    \u2506 152   \u2506 3700.662252 \u2506 2850 \u2506 4775 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Average Bill Length (Species + Island) ---\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 island    \u2506 avg_bill  \u2502\n\u2502 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 str       \u2506 str       \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Chinstrap \u2506 Dream     \u2506 48.833824 \u2502\n\u2502 Gentoo    \u2506 Biscoe    \u2506 47.504878 \u2502\n\u2502 Adelie    \u2506 Biscoe    \u2506 38.975    \u2502\n\u2502 Adelie    \u2506 Torgersen \u2506 38.95098  \u2502\n\u2502 Adelie    \u2506 Dream     \u2506 38.501786 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Body Mass Spread by Species ---\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 spread \u2502\n\u2502 ---       \u2506 ---    \u2502\n\u2502 str       \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Chinstrap \u2506 2100   \u2502\n\u2502 Adelie    \u2506 1925   \u2502\n\u2502 Gentoo    \u2506 2350   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "transform_aggregate_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd # For fallback mock data\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\n# DuckDB can query the CSV URL directly!\ntry:\n    # Create an in-memory connection\n    con = duckdb.connect()\n    # Check if we can reach the URL\n    con.sql(f\"CREATE TABLE penguins AS SELECT * FROM read_csv_auto('{URL}')\")\n    print(f\"Loaded penguins via URL into DuckDB.\")\nexcept Exception as e:\n    print(f\"URL load failed: {e}. Using mock data fallback.\")\n    # Fallback to pandas then load into DuckDB\n    data = {\n        'species': ['Adelie', 'Adelie', 'Chinstrap', 'Gentoo', 'Gentoo'],\n        'island': ['Torgersen', 'Torgersen', 'Dream', 'Biscoe', 'Biscoe'],\n        'bill_length_mm': [39.1, 39.5, 46.5, 46.1, 50.0],\n        'body_mass_g': [3750, 3800, 3500, 4500, 5700]\n    }\n    df = pd.DataFrame(data)\n    con = duckdb.connect()\n    con.register('penguins', df)\n\n# ---------------------------------------------------------\n# 1. Basic Aggregation (Mean Mass by Species)\n# ---------------------------------------------------------\nprint(\"\\n--- Average Body Mass by Species ---\")\nresult1 = con.sql(\"\"\"\n    SELECT \n        species, \n        AVG(body_mass_g) as avg_mass \n    FROM penguins \n    GROUP BY species\n\"\"\").df()\nprint(result1)\n\n# ---------------------------------------------------------\n# 2. Multiple Aggregations\n# ---------------------------------------------------------\nprint(\"\\n--- Summary Statistics by Species ---\")\nresult2 = con.sql(\"\"\"\n    SELECT \n        species, \n        COUNT(*) as count,\n        AVG(body_mass_g) as mean,\n        MIN(body_mass_g) as min,\n        MAX(body_mass_g) as max\n    FROM penguins \n    GROUP BY species\n\"\"\").df()\nprint(result2)\n\n# ---------------------------------------------------------\n# 3. Grouping by Multiple Columns\n# ---------------------------------------------------------\nprint(\"\\n--- Average Bill Length (Species + Island) ---\")\nresult3 = con.sql(\"\"\"\n    SELECT \n        species, \n        island, \n        AVG(bill_length_mm) as avg_bill\n    FROM penguins \n    GROUP BY species, island\n    ORDER BY avg_bill DESC\n\"\"\").df()\nprint(result3.head(5))\n\n# ---------------------------------------------------------\n# 4. Conditional Aggregation (SQL Filter/Case)\n# ---------------------------------------------------------\n# Count heavy penguins vs others per species\nprint(\"\\n--- Heavy vs Normal Penguins per Species ---\")\nresult4 = con.sql(\"\"\"\n    SELECT \n        species,\n        COUNT(*) FILTER (WHERE body_mass_g > 4000) as heavy_count,\n        COUNT(*) FILTER (WHERE body_mass_g <= 4000) as normal_count\n    FROM penguins \n    GROUP BY species\n\"\"\").df()\nprint(result4)",
    "language": "python",
    "output": "Loaded penguins via URL into DuckDB.\n\n--- Average Body Mass by Species ---\n     species     avg_mass\n0  Chinstrap  3733.088235\n1     Gentoo  5076.016260\n2     Adelie  3700.662252\n\n--- Summary Statistics by Species ---\n     species  count         mean   min   max\n0     Gentoo    124  5076.016260  3950  6300\n1  Chinstrap     68  3733.088235  2700  4800\n2     Adelie    152  3700.662252  2850  4775\n\n--- Average Bill Length (Species + Island) ---\n     species     island   avg_bill\n0  Chinstrap      Dream  48.833824\n1     Gentoo     Biscoe  47.504878\n2     Adelie     Biscoe  38.975000\n3     Adelie  Torgersen  38.950980\n4     Adelie      Dream  38.501786\n\n--- Heavy vs Normal Penguins per Species ---\n     species  heavy_count  normal_count\n0     Adelie           35           116\n1  Chinstrap           15            53\n2     Gentoo          122             1\n"
  },
  "transform_aggregate_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nfrom unittest.mock import MagicMock\n\n# ---------------------------------------------------------\n# Mocking BigQuery Client for Portability\n# ---------------------------------------------------------\nclass MockBQClient:\n    def __init__(self):\n        print(\"--- BigQuery Client Initialized (Mock) ---\")\n    \n    def query(self, sql):\n        print(f\"\\nExecuting SQL:\\n{sql}\")\n        \n        # Determine which result to return based on SQL content\n        if \"AVG(body_mass_g)\" in sql and \"GROUP BY species\" in sql and \"COUNT(*)\" not in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n                'avg_mass': [3700.6, 3733.1, 5076.0]\n            }))\n        elif \"COUNT(*)\" in sql and \"MIN(body_mass_g)\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n                'count': [152, 68, 124],\n                'mean': [3700.6, 3733.1, 5076.0],\n                'min': [2850.0, 2700.0, 3950.0],\n                'max': [4775.0, 4800.0, 6300.0]\n            }))\n        elif \"island\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Gentoo', 'Gentoo', 'Adelie'],\n                'island': ['Biscoe', 'Dream', 'Torgersen'],\n                'avg_bill': [47.5, 46.2, 38.8]\n            }))\n        else:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({'status': ['OK']}))\n\nclient = MockBQClient()\n\n# ---------------------------------------------------------\n# 1. Basic Aggregation (Mean Mass by Species)\n# ---------------------------------------------------------\nsql1 = \"\"\"\nSELECT \n    species, \n    AVG(body_mass_g) as avg_mass \nFROM `my-project.dataset.penguins` \nGROUP BY species\n\"\"\"\ndf1 = client.query(sql1).to_dataframe()\nprint(\"\\n--- Average Body Mass by Species ---\")\nprint(df1)\n\n# ---------------------------------------------------------\n# 2. Multiple Aggregations\n# ---------------------------------------------------------\nsql2 = \"\"\"\nSELECT \n    species, \n    COUNT(*) as count,\n    AVG(body_mass_g) as mean,\n    MIN(body_mass_g) as min,\n    MAX(body_mass_g) as max\nFROM `my-project.dataset.penguins` \nGROUP BY species\n\"\"\"\ndf2 = client.query(sql2).to_dataframe()\nprint(\"\\n--- Summary Statistics by Species ---\")\nprint(df2)\n\n# ---------------------------------------------------------\n# 3. Grouping by Multiple Columns\n# ---------------------------------------------------------\nsql3 = \"\"\"\nSELECT \n    species, \n    island, \n    AVG(bill_length_mm) as avg_bill\nFROM `my-project.dataset.penguins` \nGROUP BY species, island\nORDER BY avg_bill DESC\n\"\"\"\ndf3 = client.query(sql3).to_dataframe()\nprint(\"\\n--- Average Bill Length (Species + Island) ---\")\nprint(df3)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\nSELECT \n    species, \n    AVG(body_mass_g) as avg_mass \nFROM `my-project.dataset.penguins` \nGROUP BY species\n\n\n--- Average Body Mass by Species ---\n     species  avg_mass\n0     Adelie    3700.6\n1  Chinstrap    3733.1\n2     Gentoo    5076.0\n\nExecuting SQL:\n\nSELECT \n    species, \n    COUNT(*) as count,\n    AVG(body_mass_g) as mean,\n    MIN(body_mass_g) as min,\n    MAX(body_mass_g) as max\nFROM `my-project.dataset.penguins` \nGROUP BY species\n\n\n--- Summary Statistics by Species ---\n     species  count    mean     min     max\n0     Adelie    152  3700.6  2850.0  4775.0\n1  Chinstrap     68  3733.1  2700.0  4800.0\n2     Gentoo    124  5076.0  3950.0  6300.0\n\nExecuting SQL:\n\nSELECT \n    species, \n    island, \n    AVG(bill_length_mm) as avg_bill\nFROM `my-project.dataset.penguins` \nGROUP BY species, island\nORDER BY avg_bill DESC\n\n\n--- Average Bill Length (Species + Island) ---\n  species     island  avg_bill\n0  Gentoo     Biscoe      47.5\n1  Gentoo      Dream      46.2\n2  Adelie  Torgersen      38.8\n"
  },
  "transform_pivot_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Dream,40.3,18.0,195,3250,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Pivot Table (Long \u2192 Wide)\n# ---------------------------------------------------------\n# Create a summary: average body mass by species and island\npivot = df.pivot_table(\n    values='body_mass_g',\n    index='species',\n    columns='island',\n    aggfunc='mean'\n)\nprint(\"\\n--- Pivot Table: Avg Body Mass (Species x Island) ---\")\nprint(pivot.round(0))\n\n# ---------------------------------------------------------\n# 2. Melt (Wide \u2192 Long)\n# ---------------------------------------------------------\n# First create a wide dataframe\nwide_df = df.groupby('species')[['bill_length_mm', 'bill_depth_mm']].mean().reset_index()\nprint(\"\\n--- Wide DataFrame ---\")\nprint(wide_df.round(1))\n\n# Now melt it back to long format\nlong_df = wide_df.melt(\n    id_vars=['species'],\n    value_vars=['bill_length_mm', 'bill_depth_mm'],\n    var_name='measurement',\n    value_name='value'\n)\nprint(\"\\n--- Melted (Long) DataFrame ---\")\nprint(long_df.round(1))\n\n# ---------------------------------------------------------\n# 3. Stack / Unstack\n# ---------------------------------------------------------\n# Multi-index example\nmulti_idx = df.groupby(['species', 'island'])['body_mass_g'].mean()\nprint(\"\\n--- Multi-Index Series (Stacked) ---\")\nprint(multi_idx.head(6).round(0))\n\n# Unstack to make it wide\nunstacked = multi_idx.unstack()\nprint(\"\\n--- Unstacked (Wide) ---\")\nprint(unstacked.round(0))",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Pivot Table: Avg Body Mass (Species x Island) ---\nisland     Biscoe   Dream  Torgersen\nspecies                             \nAdelie     3710.0  3688.0     3706.0\nChinstrap     NaN  3733.0        NaN\nGentoo     5076.0     NaN        NaN\n\n--- Wide DataFrame ---\n     species  bill_length_mm  bill_depth_mm\n0     Adelie            38.8           18.3\n1  Chinstrap            48.8           18.4\n2     Gentoo            47.5           15.0\n\n--- Melted (Long) DataFrame ---\n     species     measurement  value\n0     Adelie  bill_length_mm   38.8\n1  Chinstrap  bill_length_mm   48.8\n2     Gentoo  bill_length_mm   47.5\n3     Adelie   bill_depth_mm   18.3\n4  Chinstrap   bill_depth_mm   18.4\n5     Gentoo   bill_depth_mm   15.0\n\n--- Multi-Index Series (Stacked) ---\nspecies    island   \nAdelie     Biscoe       3710.0\n           Dream        3688.0\n           Torgersen    3706.0\nChinstrap  Dream        3733.0\nGentoo     Biscoe       5076.0\nName: body_mass_g, dtype: float64\n\n--- Unstacked (Wide) ---\nisland     Biscoe   Dream  Torgersen\nspecies                             \nAdelie     3710.0  3688.0     3706.0\nChinstrap     NaN  3733.0        NaN\nGentoo     5076.0     NaN        NaN\n"
  },
  "transform_pivot_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pl.read_csv(URL)\nexcept Exception:\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Dream,40.3,18.0,195,3250,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\nprint(f\"Loaded {len(df)} rows.\")\n\n# ---------------------------------------------------------\n# 1. Pivot (Long \u2192 Wide)\n# ---------------------------------------------------------\n# Average body mass by species and island\npivot = df.pivot(\n    on=\"island\",\n    index=\"species\",\n    values=\"body_mass_g\",\n    aggregate_function=\"mean\"\n)\nprint(\"\\n--- Pivot: Avg Body Mass (Species x Island) ---\")\nprint(pivot)\n\n# ---------------------------------------------------------\n# 2. Unpivot / Melt (Wide \u2192 Long)\n# ---------------------------------------------------------\n# First create a wide aggregated frame\nwide_df = df.group_by(\"species\").agg([\n    pl.col(\"bill_length_mm\").mean().alias(\"bill_length\"),\n    pl.col(\"bill_depth_mm\").mean().alias(\"bill_depth\")\n])\nprint(\"\\n--- Wide DataFrame ---\")\nprint(wide_df)\n\n# Unpivot (melt) to long format\nlong_df = wide_df.unpivot(\n    on=[\"bill_length\", \"bill_depth\"],\n    index=\"species\",\n    variable_name=\"measurement\",\n    value_name=\"value\"\n)\nprint(\"\\n--- Unpivoted (Long) DataFrame ---\")\nprint(long_df)\n\n# ---------------------------------------------------------\n# 3. Transpose\n# ---------------------------------------------------------\n# Transpose switches rows and columns\ntransposed = wide_df.transpose(include_header=True, header_name=\"species\")\nprint(\"\\n--- Transposed ---\")\nprint(transposed)",
    "language": "python",
    "output": "Loaded 344 rows.\n\n--- Pivot: Avg Body Mass (Species x Island) ---\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 Torgersen   \u2506 Biscoe      \u2506 Dream       \u2502\n\u2502 ---       \u2506 ---         \u2506 ---         \u2506 ---         \u2502\n\u2502 str       \u2506 f64         \u2506 f64         \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie    \u2506 3706.372549 \u2506 3709.659091 \u2506 3688.392857 \u2502\n\u2502 Chinstrap \u2506 null        \u2506 null        \u2506 3733.088235 \u2502\n\u2502 Gentoo    \u2506 null        \u2506 5076.01626  \u2506 null        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Wide DataFrame ---\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 bill_length \u2506 bill_depth \u2502\n\u2502 ---       \u2506 ---         \u2506 ---        \u2502\n\u2502 str       \u2506 f64         \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie    \u2506 38.791391   \u2506 18.346358  \u2502\n\u2502 Gentoo    \u2506 47.504878   \u2506 14.982114  \u2502\n\u2502 Chinstrap \u2506 48.833824   \u2506 18.420588  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Unpivoted (Long) DataFrame ---\nshape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species   \u2506 measurement \u2506 value     \u2502\n\u2502 ---       \u2506 ---         \u2506 ---       \u2502\n\u2502 str       \u2506 str         \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie    \u2506 bill_length \u2506 38.791391 \u2502\n\u2502 Gentoo    \u2506 bill_length \u2506 47.504878 \u2502\n\u2502 Chinstrap \u2506 bill_length \u2506 48.833824 \u2502\n\u2502 Adelie    \u2506 bill_depth  \u2506 18.346358 \u2502\n\u2502 Gentoo    \u2506 bill_depth  \u2506 14.982114 \u2502\n\u2502 Chinstrap \u2506 bill_depth  \u2506 18.420588 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Transposed ---\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species     \u2506 column_0           \u2506 column_1           \u2506 column_2           \u2502\n\u2502 ---         \u2506 ---                \u2506 ---                \u2506 ---                \u2502\n\u2502 str         \u2506 str                \u2506 str                \u2506 str                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 species     \u2506 Adelie             \u2506 Gentoo             \u2506 Chinstrap          \u2502\n\u2502 bill_length \u2506 38.79139072847682  \u2506 47.50487804878049  \u2506 48.83382352941176  \u2502\n\u2502 bill_depth  \u2506 18.346357615894043 \u2506 14.982113821138212 \u2506 18.420588235294115 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "transform_pivot_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ncon = duckdb.connect()\ntry:\n    con.sql(f\"CREATE TABLE penguins AS SELECT * FROM read_csv_auto('{URL}')\")\n    print(\"Loaded penguins via URL into DuckDB.\")\nexcept Exception as e:\n    print(f\"URL load failed: {e}. Using mock data fallback.\")\n    data = {\n        'species': ['Adelie', 'Adelie', 'Adelie', 'Chinstrap', 'Gentoo', 'Gentoo'],\n        'island': ['Torgersen', 'Torgersen', 'Dream', 'Dream', 'Biscoe', 'Biscoe'],\n        'bill_length_mm': [39.1, 39.5, 40.3, 46.5, 46.1, 50.0],\n        'body_mass_g': [3750, 3800, 3250, 3500, 4500, 5700]\n    }\n    df = pd.DataFrame(data)\n    con.register('penguins', df)\n\n# ---------------------------------------------------------\n# 1. PIVOT (Long \u2192 Wide)\n# ---------------------------------------------------------\n# DuckDB has native PIVOT syntax\nprint(\"\\n--- PIVOT: Avg Body Mass (Species x Island) ---\")\npivot_result = con.sql(\"\"\"\n    PIVOT penguins \n    ON island \n    USING AVG(body_mass_g) \n    GROUP BY species\n\"\"\").df()\nprint(pivot_result)\n\n# ---------------------------------------------------------\n# 2. UNPIVOT (Wide \u2192 Long)\n# ---------------------------------------------------------\n# First create a wide summary table\ncon.sql(\"\"\"\n    CREATE OR REPLACE TABLE wide_summary AS\n    SELECT \n        species, \n        AVG(bill_length_mm) as bill_length,\n        AVG(body_mass_g) as body_mass\n    FROM penguins\n    GROUP BY species\n\"\"\")\n\nprint(\"\\n--- Wide Summary ---\")\nprint(con.sql(\"SELECT * FROM wide_summary\").df())\n\n# Unpivot it\nprint(\"\\n--- UNPIVOT Result ---\")\nunpivot_result = con.sql(\"\"\"\n    SELECT species, metric, value FROM (\n        SELECT species, \n               CAST(bill_length AS DOUBLE) as bill_length, \n               CAST(body_mass AS DOUBLE) as body_mass \n        FROM wide_summary\n    ) UNPIVOT INCLUDE NULLS (\n        value FOR metric IN (bill_length, body_mass)\n    )\n\"\"\").df()\nprint(unpivot_result)\n\n# ---------------------------------------------------------\n# 3. Crosstab with CASE WHEN (Manual Pivot)\n# ---------------------------------------------------------\n# More flexible approach using CASE WHEN\nprint(\"\\n--- Manual Pivot with CASE WHEN ---\")\nmanual_pivot = con.sql(\"\"\"\n    SELECT \n        species,\n        AVG(CASE WHEN island = 'Biscoe' THEN body_mass_g END) as Biscoe,\n        AVG(CASE WHEN island = 'Dream' THEN body_mass_g END) as Dream,\n        AVG(CASE WHEN island = 'Torgersen' THEN body_mass_g END) as Torgersen\n    FROM penguins\n    GROUP BY species\n\"\"\").df()\nprint(manual_pivot)",
    "language": "python",
    "output": "Loaded penguins via URL into DuckDB.\n\n--- PIVOT: Avg Body Mass (Species x Island) ---\n     species       Biscoe        Dream    Torgersen\n0     Gentoo  5076.016260          NaN          NaN\n1  Chinstrap          NaN  3733.088235          NaN\n2     Adelie  3709.659091  3688.392857  3706.372549\n\n--- Wide Summary ---\n     species  bill_length    body_mass\n0     Adelie    38.791391  3700.662252\n1     Gentoo    47.504878  5076.016260\n2  Chinstrap    48.833824  3733.088235\n\n--- UNPIVOT Result ---\n     species       metric        value\n0     Adelie  bill_length    38.791391\n1     Adelie    body_mass  3700.662252\n2     Gentoo  bill_length    47.504878\n3     Gentoo    body_mass  5076.016260\n4  Chinstrap  bill_length    48.833824\n5  Chinstrap    body_mass  3733.088235\n\n--- Manual Pivot with CASE WHEN ---\n     species       Biscoe        Dream    Torgersen\n0     Gentoo  5076.016260          NaN          NaN\n1     Adelie  3709.659091  3688.392857  3706.372549\n2  Chinstrap          NaN  3733.088235          NaN\n"
  },
  "transform_pivot_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nfrom unittest.mock import MagicMock\n\n# ---------------------------------------------------------\n# Mocking BigQuery Client for Portability\n# ---------------------------------------------------------\nclass MockBQClient:\n    def __init__(self):\n        print(\"--- BigQuery Client Initialized (Mock) ---\")\n    \n    def query(self, sql):\n        print(f\"\\nExecuting SQL:\\n{sql}\")\n        \n        # Return mock results based on query type\n        if \"PIVOT\" in sql.upper():\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n                'Biscoe': [3700.0, None, 5076.0],\n                'Dream': [3688.0, 3733.0, None],\n                'Torgersen': [3706.0, None, None]\n            }))\n        elif \"UNPIVOT\" in sql.upper():\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Adelie', 'Chinstrap', 'Chinstrap', 'Gentoo', 'Gentoo'],\n                'metric': ['bill_length', 'body_mass', 'bill_length', 'body_mass', 'bill_length', 'body_mass'],\n                'value': [38.8, 3700.0, 48.8, 3733.0, 47.5, 5076.0]\n            }))\n        else:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({'status': ['OK']}))\n\nclient = MockBQClient()\n\n# ---------------------------------------------------------\n# 1. BigQuery PIVOT\n# ---------------------------------------------------------\nsql_pivot = \"\"\"\nSELECT * FROM (\n    SELECT species, island, body_mass_g\n    FROM `my-project.dataset.penguins`\n)\nPIVOT (\n    AVG(body_mass_g) FOR island IN ('Biscoe', 'Dream', 'Torgersen')\n)\n\"\"\"\ndf_pivot = client.query(sql_pivot).to_dataframe()\nprint(\"\\n--- PIVOT Result ---\")\nprint(df_pivot)\n\n# ---------------------------------------------------------\n# 2. BigQuery UNPIVOT\n# ---------------------------------------------------------\nsql_unpivot = \"\"\"\nSELECT * FROM (\n    SELECT species, bill_length, body_mass\n    FROM `my-project.dataset.penguin_summary`\n)\nUNPIVOT (\n    value FOR metric IN (bill_length, body_mass)\n)\n\"\"\"\ndf_unpivot = client.query(sql_unpivot).to_dataframe()\nprint(\"\\n--- UNPIVOT Result ---\")\nprint(df_unpivot)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\nSELECT * FROM (\n    SELECT species, island, body_mass_g\n    FROM `my-project.dataset.penguins`\n)\nPIVOT (\n    AVG(body_mass_g) FOR island IN ('Biscoe', 'Dream', 'Torgersen')\n)\n\n\n--- PIVOT Result ---\n     species  Biscoe   Dream  Torgersen\n0     Adelie  3700.0  3688.0     3706.0\n1  Chinstrap     NaN  3733.0        NaN\n2     Gentoo  5076.0     NaN        NaN\n\nExecuting SQL:\n\nSELECT * FROM (\n    SELECT species, bill_length, body_mass\n    FROM `my-project.dataset.penguin_summary`\n)\nUNPIVOT (\n    value FOR metric IN (bill_length, body_mass)\n)\n\n\n--- UNPIVOT Result ---\n     species  Biscoe   Dream  Torgersen\n0     Adelie  3700.0  3688.0     3706.0\n1  Chinstrap     NaN  3733.0        NaN\n2     Gentoo  5076.0     NaN        NaN\n"
  },
  "transform_window_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pd.read_csv(URL)\nexcept Exception:\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Dream,40.3,18.0,195,3250,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pd.read_csv(io.StringIO(data))\n\n# Clean up for demo\ndf = df.dropna(subset=['body_mass_g']).head(20)\nprint(f\"Using {len(df)} rows for demo.\")\n\n# ---------------------------------------------------------\n# 1. RANK: Ranking within Groups\n# ---------------------------------------------------------\n# Rank penguins by body mass within each species\ndf['mass_rank'] = df.groupby('species')['body_mass_g'].rank(ascending=False, method='dense')\nprint(\"\\n--- Rank by Body Mass (within Species) ---\")\nprint(df[['species', 'body_mass_g', 'mass_rank']].head(8))\n\n# ---------------------------------------------------------\n# 2. ROW_NUMBER: Sequential numbering\n# ---------------------------------------------------------\n# Row number within each species (ordered by bill length)\ndf = df.sort_values(['species', 'bill_length_mm'])\ndf['row_num'] = df.groupby('species').cumcount() + 1\nprint(\"\\n--- Row Number (within Species, by Bill Length) ---\")\nprint(df[['species', 'bill_length_mm', 'row_num']].head(8))\n\n# ---------------------------------------------------------\n# 3. LAG / SHIFT: Access previous rows\n# ---------------------------------------------------------\n# Get the previous penguin's body mass within the same species\ndf = df.sort_values(['species', 'body_mass_g'])\ndf['prev_mass'] = df.groupby('species')['body_mass_g'].shift(1)\ndf['mass_diff'] = df['body_mass_g'] - df['prev_mass']\nprint(\"\\n--- LAG (Previous Mass) and Difference ---\")\nprint(df[['species', 'body_mass_g', 'prev_mass', 'mass_diff']].head(8))\n\n# ---------------------------------------------------------\n# 4. CUMSUM: Running Totals\n# ---------------------------------------------------------\n# Cumulative count within each species\ndf['running_count'] = df.groupby('species').cumcount() + 1\nprint(\"\\n--- Cumulative Count (within Species) ---\")\nprint(df[['species', 'body_mass_g', 'running_count']].head(8))",
    "language": "python",
    "output": "Using 20 rows for demo.\n\n--- Rank by Body Mass (within Species) ---\n  species  body_mass_g  mass_rank\n0  Adelie       3750.0        7.0\n1  Adelie       3800.0        6.0\n2  Adelie       3250.0       16.0\n4  Adelie       3450.0       12.0\n5  Adelie       3650.0        9.0\n6  Adelie       3625.0       10.0\n7  Adelie       4675.0        1.0\n8  Adelie       3475.0       11.0\n\n--- Row Number (within Species, by Bill Length) ---\n   species  bill_length_mm  row_num\n8   Adelie            34.1        1\n18  Adelie            34.4        2\n14  Adelie            34.6        3\n15  Adelie            36.6        4\n4   Adelie            36.7        5\n10  Adelie            37.8        6\n11  Adelie            37.8        7\n20  Adelie            37.8        8\n\n--- LAG (Previous Mass) and Difference ---\n   species  body_mass_g  prev_mass  mass_diff\n12  Adelie       3200.0        NaN        NaN\n2   Adelie       3250.0     3200.0       50.0\n10  Adelie       3300.0     3250.0       50.0\n18  Adelie       3325.0     3300.0       25.0\n20  Adelie       3400.0     3325.0       75.0\n4   Adelie       3450.0     3400.0       50.0\n16  Adelie       3450.0     3450.0        0.0\n8   Adelie       3475.0     3450.0       25.0\n\n--- Cumulative Count (within Species) ---\n   species  body_mass_g  running_count\n12  Adelie       3200.0              1\n2   Adelie       3250.0              2\n10  Adelie       3300.0              3\n18  Adelie       3325.0              4\n20  Adelie       3400.0              5\n4   Adelie       3450.0              6\n16  Adelie       3450.0              7\n8   Adelie       3475.0              8\n"
  },
  "transform_window_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\nimport polars as pl\nimport io\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ntry:\n    df = pl.read_csv(URL)\nexcept Exception:\n    data = \"\"\"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,Male\nAdelie,Torgersen,39.5,17.4,186,3800,Female\nAdelie,Dream,40.3,18.0,195,3250,Female\nChinstrap,Dream,46.5,17.9,192,3500,Female\nGentoo,Biscoe,46.1,13.2,211,4500,Female\nGentoo,Biscoe,50.0,16.3,230,5700,Male\n\"\"\"\n    df = pl.read_csv(io.StringIO(data))\n\n# Clean up for demo\ndf = df.drop_nulls(subset=[\"body_mass_g\"]).head(20)\nprint(f\"Using {len(df)} rows for demo.\")\n\n# ---------------------------------------------------------\n# 1. RANK: Ranking within Groups\n# ---------------------------------------------------------\n# Polars uses .over() for window functions\nresult = df.with_columns(\n    pl.col(\"body_mass_g\")\n    .rank(method=\"dense\", descending=True)\n    .over(\"species\")\n    .alias(\"mass_rank\")\n)\nprint(\"\\n--- Rank by Body Mass (within Species) ---\")\nprint(result.select([\"species\", \"body_mass_g\", \"mass_rank\"]).head(8))\n\n# ---------------------------------------------------------\n# 2. ROW_NUMBER: Sequential numbering\n# ---------------------------------------------------------\nresult = df.sort(\"bill_length_mm\").with_columns(\n    pl.lit(1).cum_sum().over(\"species\").alias(\"row_num\")\n)\nprint(\"\\n--- Row Number (within Species, by Bill Length) ---\")\nprint(result.select([\"species\", \"bill_length_mm\", \"row_num\"]).head(8))\n\n# ---------------------------------------------------------\n# 3. LAG / SHIFT: Access previous rows\n# ---------------------------------------------------------\nresult = df.sort([\"species\", \"body_mass_g\"]).with_columns(\n    pl.col(\"body_mass_g\").shift(1).over(\"species\").alias(\"prev_mass\")\n).with_columns(\n    (pl.col(\"body_mass_g\") - pl.col(\"prev_mass\")).alias(\"mass_diff\")\n)\nprint(\"\\n--- LAG (Previous Mass) and Difference ---\")\nprint(result.select([\"species\", \"body_mass_g\", \"prev_mass\", \"mass_diff\"]).head(8))\n\n# ---------------------------------------------------------\n# 4. Running Aggregates\n# ---------------------------------------------------------\nresult = df.sort([\"species\", \"body_mass_g\"]).with_columns(\n    pl.col(\"body_mass_g\").cum_sum().over(\"species\").alias(\"running_mass\")\n)\nprint(\"\\n--- Cumulative Sum of Body Mass (within Species) ---\")\nprint(result.select([\"species\", \"body_mass_g\", \"running_mass\"]).head(8))",
    "language": "python",
    "output": "Using 20 rows for demo.\n\n--- Rank by Body Mass (within Species) ---\nshape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 body_mass_g \u2506 mass_rank \u2502\n\u2502 ---     \u2506 ---         \u2506 ---       \u2502\n\u2502 str     \u2506 i64         \u2506 u32       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 3750        \u2506 7         \u2502\n\u2502 Adelie  \u2506 3800        \u2506 6         \u2502\n\u2502 Adelie  \u2506 3250        \u2506 16        \u2502\n\u2502 Adelie  \u2506 3450        \u2506 12        \u2502\n\u2502 Adelie  \u2506 3650        \u2506 9         \u2502\n\u2502 Adelie  \u2506 3625        \u2506 10        \u2502\n\u2502 Adelie  \u2506 4675        \u2506 1         \u2502\n\u2502 Adelie  \u2506 3475        \u2506 11        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Row Number (within Species, by Bill Length) ---\nshape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 bill_length_mm \u2506 row_num    \u2502\n\u2502 ---     \u2506 ---            \u2506 ---        \u2502\n\u2502 str     \u2506 f64            \u2506 i32        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 34.1           \u2506 1          \u2502\n\u2502 Adelie  \u2506 34.4           \u2506 0          \u2502\n\u2502 Adelie  \u2506 34.6           \u2506 0          \u2502\n\u2502 Adelie  \u2506 36.6           \u2506 0          \u2502\n\u2502 Adelie  \u2506 36.7           \u2506 1          \u2502\n\u2502 Adelie  \u2506 37.8           \u2506 0          \u2502\n\u2502 Adelie  \u2506 37.8           \u2506 1614971088 \u2502\n\u2502 Adelie  \u2506 37.8           \u2506 32644      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- LAG (Previous Mass) and Difference ---\nshape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 body_mass_g \u2506 prev_mass \u2506 mass_diff \u2502\n\u2502 ---     \u2506 ---         \u2506 ---       \u2506 ---       \u2502\n\u2502 str     \u2506 i64         \u2506 i64       \u2506 i64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 3200        \u2506 null      \u2506 null      \u2502\n\u2502 Adelie  \u2506 3250        \u2506 3200      \u2506 50        \u2502\n\u2502 Adelie  \u2506 3300        \u2506 3250      \u2506 50        \u2502\n\u2502 Adelie  \u2506 3325        \u2506 3300      \u2506 25        \u2502\n\u2502 Adelie  \u2506 3400        \u2506 3325      \u2506 75        \u2502\n\u2502 Adelie  \u2506 3450        \u2506 3400      \u2506 50        \u2502\n\u2502 Adelie  \u2506 3450        \u2506 3450      \u2506 0         \u2502\n\u2502 Adelie  \u2506 3475        \u2506 3450      \u2506 25        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n--- Cumulative Sum of Body Mass (within Species) ---\nshape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 body_mass_g \u2506 running_mass \u2502\n\u2502 ---     \u2506 ---         \u2506 ---          \u2502\n\u2502 str     \u2506 i64         \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Adelie  \u2506 3200        \u2506 3200         \u2502\n\u2502 Adelie  \u2506 3250        \u2506 6450         \u2502\n\u2502 Adelie  \u2506 3300        \u2506 9750         \u2502\n\u2502 Adelie  \u2506 3325        \u2506 13075        \u2502\n\u2502 Adelie  \u2506 3400        \u2506 16475        \u2502\n\u2502 Adelie  \u2506 3450        \u2506 19925        \u2502\n\u2502 Adelie  \u2506 3450        \u2506 23375        \u2502\n\u2502 Adelie  \u2506 3475        \u2506 26850        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
  },
  "transform_window_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport pandas as pd\n\n# ---------------------------------------------------------\n# Load Dataset (Palmer Penguins)\n# ---------------------------------------------------------\nURL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n\ncon = duckdb.connect()\ntry:\n    con.sql(f\"CREATE TABLE penguins AS SELECT * FROM read_csv_auto('{URL}') LIMIT 20\")\n    print(\"Loaded 20 penguins via URL into DuckDB.\")\nexcept Exception as e:\n    print(f\"URL load failed: {e}. Using mock data fallback.\")\n    data = {\n        'species': ['Adelie', 'Adelie', 'Adelie', 'Chinstrap', 'Gentoo', 'Gentoo'],\n        'island': ['Torgersen', 'Torgersen', 'Dream', 'Dream', 'Biscoe', 'Biscoe'],\n        'bill_length_mm': [39.1, 39.5, 40.3, 46.5, 46.1, 50.0],\n        'body_mass_g': [3750, 3800, 3250, 3500, 4500, 5700]\n    }\n    df = pd.DataFrame(data)\n    con.register('penguins', df)\n\n# ---------------------------------------------------------\n# 1. RANK: Ranking within Groups\n# ---------------------------------------------------------\nprint(\"\\n--- RANK by Body Mass (within Species) ---\")\nrank_result = con.sql(\"\"\"\n    SELECT \n        species, body_mass_g,\n        RANK() OVER (PARTITION BY species ORDER BY body_mass_g DESC) as mass_rank\n    FROM penguins\n    ORDER BY species, mass_rank\n    LIMIT 8\n\"\"\").df()\nprint(rank_result)\n\n# ---------------------------------------------------------\n# 2. ROW_NUMBER: Sequential numbering\n# ---------------------------------------------------------\nprint(\"\\n--- ROW_NUMBER (within Species, by Bill Length) ---\")\nrownum_result = con.sql(\"\"\"\n    SELECT \n        species, bill_length_mm,\n        ROW_NUMBER() OVER (PARTITION BY species ORDER BY bill_length_mm) as row_num\n    FROM penguins\n    ORDER BY species, row_num\n    LIMIT 8\n\"\"\").df()\nprint(rownum_result)\n\n# ---------------------------------------------------------\n# 3. LAG: Access previous rows\n# ---------------------------------------------------------\nprint(\"\\n--- LAG (Previous Body Mass within Species) ---\")\nlag_result = con.sql(\"\"\"\n    SELECT \n        species, body_mass_g,\n        LAG(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g) as prev_mass,\n        body_mass_g - LAG(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g) as mass_diff\n    FROM penguins\n    ORDER BY species, body_mass_g\n    LIMIT 8\n\"\"\").df()\nprint(lag_result)\n\n# ---------------------------------------------------------\n# 4. Running Totals with SUM OVER\n# ---------------------------------------------------------\nprint(\"\\n--- Running Total Body Mass (within Species) ---\")\nrunning_result = con.sql(\"\"\"\n    SELECT \n        species, body_mass_g,\n        SUM(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g \n                               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_mass\n    FROM penguins\n    ORDER BY species, body_mass_g\n    LIMIT 8\n\"\"\").df()\nprint(running_result)",
    "language": "python",
    "output": "Loaded 20 penguins via URL into DuckDB.\n\n--- RANK by Body Mass (within Species) ---\n  species  body_mass_g  mass_rank\n0  Adelie         4675          1\n1  Adelie         4500          2\n2  Adelie         4400          3\n3  Adelie         4250          4\n4  Adelie         4200          5\n5  Adelie         3800          6\n6  Adelie         3800          6\n7  Adelie         3750          8\n\n--- ROW_NUMBER (within Species, by Bill Length) ---\n  species  bill_length_mm  row_num\n0  Adelie            34.1        1\n1  Adelie            34.4        2\n2  Adelie            34.6        3\n3  Adelie            36.6        4\n4  Adelie            36.7        5\n5  Adelie            37.8        6\n6  Adelie            37.8        7\n7  Adelie            38.6        8\n\n--- LAG (Previous Body Mass within Species) ---\n  species  body_mass_g  prev_mass  mass_diff\n0  Adelie         3200       <NA>       <NA>\n1  Adelie         3250       3200         50\n2  Adelie         3300       3250         50\n3  Adelie         3325       3300         25\n4  Adelie         3450       3325        125\n5  Adelie         3450       3450          0\n6  Adelie         3475       3450         25\n7  Adelie         3625       3475        150\n\n--- Running Total Body Mass (within Species) ---\n  species  body_mass_g  running_mass\n0  Adelie         3200        3200.0\n1  Adelie         3250        6450.0\n2  Adelie         3300        9750.0\n3  Adelie         3325       13075.0\n4  Adelie         3450       16525.0\n5  Adelie         3450       19975.0\n6  Adelie         3475       23450.0\n7  Adelie         3625       27075.0\n"
  },
  "transform_window_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nfrom unittest.mock import MagicMock\n\n# ---------------------------------------------------------\n# Mocking BigQuery Client for Portability\n# ---------------------------------------------------------\nclass MockBQClient:\n    def __init__(self):\n        print(\"--- BigQuery Client Initialized (Mock) ---\")\n    \n    def query(self, sql):\n        print(f\"\\nExecuting SQL:\\n{sql[:200]}...\")\n        \n        # Return mock results based on query type\n        if \"RANK()\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo'],\n                'body_mass_g': [4775, 4725, 4700, 6300, 6050],\n                'mass_rank': [1, 2, 3, 1, 2]\n            }))\n        elif \"ROW_NUMBER()\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo'],\n                'bill_length_mm': [32.1, 33.1, 33.5, 40.9, 42.0],\n                'row_num': [1, 2, 3, 1, 2]\n            }))\n        elif \"LAG\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo'],\n                'body_mass_g': [2850, 2900, 2925, 3950, 4050],\n                'prev_mass': [None, 2850, 2900, None, 3950],\n                'mass_diff': [None, 50, 25, None, 100]\n            }))\n        elif \"SUM\" in sql and \"OVER\" in sql:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({\n                'species': ['Adelie', 'Adelie', 'Gentoo', 'Gentoo'],\n                'body_mass_g': [2850, 2900, 3950, 4050],\n                'running_mass': [2850, 5750, 3950, 8000]\n            }))\n        else:\n            return MagicMock(to_dataframe=lambda: pd.DataFrame({'status': ['OK']}))\n\nclient = MockBQClient()\n\n# ---------------------------------------------------------\n# 1. RANK: Ranking within Groups\n# ---------------------------------------------------------\nsql_rank = \"\"\"\nSELECT \n    species, body_mass_g,\n    RANK() OVER (PARTITION BY species ORDER BY body_mass_g DESC) as mass_rank\nFROM `my-project.dataset.penguins`\nORDER BY species, mass_rank\nLIMIT 5\n\"\"\"\ndf_rank = client.query(sql_rank).to_dataframe()\nprint(\"\\n--- RANK Result ---\")\nprint(df_rank)\n\n# ---------------------------------------------------------\n# 2. ROW_NUMBER\n# ---------------------------------------------------------\nsql_rownum = \"\"\"\nSELECT \n    species, bill_length_mm,\n    ROW_NUMBER() OVER (PARTITION BY species ORDER BY bill_length_mm) as row_num\nFROM `my-project.dataset.penguins`\nLIMIT 5\n\"\"\"\ndf_rownum = client.query(sql_rownum).to_dataframe()\nprint(\"\\n--- ROW_NUMBER Result ---\")\nprint(df_rownum)\n\n# ---------------------------------------------------------\n# 3. LAG\n# ---------------------------------------------------------\nsql_lag = \"\"\"\nSELECT \n    species, body_mass_g,\n    LAG(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g) as prev_mass,\n    body_mass_g - LAG(body_mass_g) OVER (...) as mass_diff\nFROM `my-project.dataset.penguins`\nLIMIT 5\n\"\"\"\ndf_lag = client.query(sql_lag).to_dataframe()\nprint(\"\\n--- LAG Result ---\")\nprint(df_lag)\n\n# ---------------------------------------------------------\n# 4. Running Total\n# ---------------------------------------------------------\nsql_running = \"\"\"\nSELECT \n    species, body_mass_g,\n    SUM(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g \n                           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_mass\nFROM `my-project.dataset.penguins`\nLIMIT 4\n\"\"\"\ndf_running = client.query(sql_running).to_dataframe()\nprint(\"\\n--- Running Total Result ---\")\nprint(df_running)",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\nExecuting SQL:\n\nSELECT \n    species, body_mass_g,\n    RANK() OVER (PARTITION BY species ORDER BY body_mass_g DESC) as mass_rank\nFROM `my-project.dataset.penguins`\nORDER BY species, mass_rank\nLIMIT 5\n...\n\n--- RANK Result ---\n  species  body_mass_g  mass_rank\n0  Adelie         4775          1\n1  Adelie         4725          2\n2  Adelie         4700          3\n3  Gentoo         6300          1\n4  Gentoo         6050          2\n\nExecuting SQL:\n\nSELECT \n    species, bill_length_mm,\n    ROW_NUMBER() OVER (PARTITION BY species ORDER BY bill_length_mm) as row_num\nFROM `my-project.dataset.penguins`\nLIMIT 5\n...\n\n--- ROW_NUMBER Result ---\n  species  bill_length_mm  row_num\n0  Adelie            32.1        1\n1  Adelie            33.1        2\n2  Adelie            33.5        3\n3  Gentoo            40.9        1\n4  Gentoo            42.0        2\n\nExecuting SQL:\n\nSELECT \n    species, body_mass_g,\n    LAG(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g) as prev_mass,\n    body_mass_g - LAG(body_mass_g) OVER (...) as mass_diff\nFROM `my-project.datas...\n\n--- LAG Result ---\n  species  body_mass_g  prev_mass  mass_diff\n0  Adelie         2850        NaN        NaN\n1  Adelie         2900     2850.0       50.0\n2  Adelie         2925     2900.0       25.0\n3  Gentoo         3950        NaN        NaN\n4  Gentoo         4050     3950.0      100.0\n\nExecuting SQL:\n\nSELECT \n    species, body_mass_g,\n    SUM(body_mass_g) OVER (PARTITION BY species ORDER BY body_mass_g \n                           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_mass\nFR...\n\n--- Running Total Result ---\n  species  body_mass_g  running_mass\n0  Adelie         2850          2850\n1  Adelie         2900          5750\n2  Gentoo         3950          3950\n3  Gentoo         4050          8000\n"
  },
  "output_files_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\nimport pandas as pd\nimport os\n\n# Create a sample summary dataframe\ndata = {\n    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n    'avg_mass_g': [3701, 3733, 5076],\n    'count': [152, 68, 124]\n}\ndf = pd.DataFrame(data)\n\n# 1. Save to CSV\ncsv_path = \"penguins_summary.csv\"\ndf.to_csv(csv_path, index=False)\nprint(f\"\u2705 Saved to CSV: {csv_path}\")\n\n# 2. Save to Parquet (requires pyarrow or fastparquet)\nparquet_path = \"penguins_summary.parquet\"\ndf.to_parquet(parquet_path, index=False)\nprint(f\"\u2705 Saved to Parquet: {parquet_path}\")\n\n# 3. Save to JSON\njson_path = \"penguins_summary.json\"\ndf.to_json(json_path, orient='records', indent=2)\nprint(f\"\u2705 Saved to JSON: {json_path}\")\n\n# Clean up (silent in production, but good for demo)\nfor path in [csv_path, parquet_path, json_path]:\n    if os.path.exists(path):\n        os.remove(path)",
    "language": "python",
    "output": "\u2705 Saved to CSV: penguins_summary.csv\n\u2705 Saved to Parquet: penguins_summary.parquet\n\u2705 Saved to JSON: penguins_summary.json\n"
  },
  "output_files_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n# ]\n# ///\nimport polars as pl\nimport os\n\n# Create a sample summary dataframe\ndf = pl.DataFrame({\n    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n    'avg_mass_g': [3701, 3733, 5076],\n    'count': [152, 68, 124]\n})\n\n# 1. Save to CSV\ncsv_path = \"penguins_summary_pl.csv\"\ndf.write_csv(csv_path)\nprint(f\"\u2705 Saved to CSV: {csv_path}\")\n\n# 2. Save to Parquet\nparquet_path = \"penguins_summary_pl.parquet\"\ndf.write_parquet(parquet_path)\nprint(f\"\u2705 Saved to Parquet: {parquet_path}\")\n\n# 3. Save to JSON\njson_path = \"penguins_summary_pl.json\"\ndf.write_json(json_path)\nprint(f\"\u2705 Saved to JSON: {json_path}\")\n\n# Clean up\nfor path in [csv_path, parquet_path, json_path]:\n    if os.path.exists(path):\n        os.remove(path)",
    "language": "python",
    "output": "\u2705 Saved to CSV: penguins_summary_pl.csv\n\u2705 Saved to Parquet: penguins_summary_pl.parquet\n\u2705 Saved to JSON: penguins_summary_pl.json\n"
  },
  "output_files_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n# ]\n# ///\nimport duckdb\nimport os\n\ncon = duckdb.connect()\n\n# Setup dummy data in a table\ncon.sql(\"\"\"\n    CREATE TABLE summary AS \n    SELECT * FROM (VALUES \n        ('Adelie', 3701, 152), \n        ('Chinstrap', 3733, 68), \n        ('Gentoo', 5076, 124)\n    ) t(species, avg_mass_g, count)\n\"\"\")\n\n# 1. Save to CSV\ncsv_path = \"penguins_summary_db.csv\"\ncon.sql(f\"COPY summary TO '{csv_path}' (FORMAT CSV, HEADER)\")\nprint(f\"\u2705 Saved to CSV: {csv_path}\")\n\n# 2. Save to Parquet\nparquet_path = \"penguins_summary_db.parquet\"\ncon.sql(f\"COPY summary TO '{parquet_path}' (FORMAT PARQUET)\")\nprint(f\"\u2705 Saved to Parquet: {parquet_path}\")\n\n# 3. Save to JSON\njson_path = \"penguins_summary_db.json\"\ncon.sql(f\"COPY summary TO '{json_path}' (FORMAT JSON)\")\nprint(f\"\u2705 Saved to JSON: {json_path}\")\n\n# Clean up\nfor path in [csv_path, parquet_path, json_path]:\n    if os.path.exists(path):\n        os.remove(path)",
    "language": "python",
    "output": "\u2705 Saved to CSV: penguins_summary_db.csv\n\u2705 Saved to Parquet: penguins_summary_db.parquet\n\u2705 Saved to JSON: penguins_summary_db.json\n"
  },
  "output_files_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nfrom unittest.mock import MagicMock\n\n# ---------------------------------------------------------\n# Mocking BigQuery Client for Portability\n# ---------------------------------------------------------\nclass MockBQClient:\n    def __init__(self):\n        print(\"--- BigQuery Client Initialized (Mock) ---\")\n    \n    def extract_table(self, table_ref, destination_uri, job_config=None):\n        print(f\"\u2705 Extraction Job Started: {table_ref} -> {destination_uri}\")\n        mock_job = MagicMock()\n        mock_job.result = lambda: print(f\"\u2705 Extraction Job Completed!\")\n        return mock_job\n\nclient = MockBQClient()\n\n# ---------------------------------------------------------\n# BigQuery Export (Extraction)\n# ---------------------------------------------------------\n# Note: To export to files from BigQuery, you typically \"extract\" a table \n# to a Cloud Storage bucket.\n\ntable_id = \"my-project.dataset.summary\"\ndestination_uri_csv = \"gs://my-bucket/penguins.csv\"\ndestination_uri_parquet = \"gs://my-bucket/penguins.parquet\"\n\n# Export to CSV\nclient.extract_table(table_id, destination_uri_csv).result()\n\n# Export to Parquet\nclient.extract_table(table_id, destination_uri_parquet).result()\n\nprint(\"\\n--- Summary of Exports (Simulated) ---\")\nprint(f\"CSV: {destination_uri_csv}\")\nprint(f\"Parquet: {destination_uri_parquet}\")",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\u2705 Extraction Job Started: my-project.dataset.summary -> gs://my-bucket/penguins.csv\n\u2705 Extraction Job Completed!\n\u2705 Extraction Job Started: my-project.dataset.summary -> gs://my-bucket/penguins.parquet\n\u2705 Extraction Job Completed!\n\n--- Summary of Exports (Simulated) ---\nCSV: gs://my-bucket/penguins.csv\nParquet: gs://my-bucket/penguins.parquet\n"
  },
  "output_database_pandas": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"pandas\",\n#     \"sqlalchemy\",\n# ]\n# ///\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport os\n\n# Create a sample summary dataframe\ndf = pd.DataFrame({\n    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n    'avg_mass_g': [3701, 3733, 5076]\n})\n\n# 1. Connect to SQLite (local file)\ndb_path = \"penguins.db\"\nengine = create_engine(f\"sqlite:///{db_path}\")\n\n# 2. Write to Database\n# - name: table name\n# - con: sqlalchemy engine\n# - if_exists: 'replace', 'append', or 'fail'\ndf.to_sql('species_summary', con=engine, if_exists='replace', index=False)\nprint(f\"\u2705 Successfully wrote {len(df)} rows to 'species_summary' table in {db_path}.\")\n\n# 3. Quick verification query\nwith engine.connect() as conn:\n    result = pd.read_sql(\"SELECT * FROM species_summary\", conn)\n    print(\"\\n--- Rows read back from DB ---\")\n    print(result)\n\n# Clean up\nif os.path.exists(db_path):\n    os.remove(db_path)",
    "language": "python",
    "output": "\u2705 Successfully wrote 3 rows to 'species_summary' table in penguins.db.\n\n--- Rows read back from DB ---\n     species  avg_mass_g\n0     Adelie        3701\n1  Chinstrap        3733\n2     Gentoo        5076\n"
  },
  "output_database_polars": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"polars\",\n#     \"adbc-driver-sqlite\",\n#     \"sqlalchemy\",\n#     \"pandas\",\n#     \"pyarrow\",\n# ]\n# ///\nimport polars as pl\nimport os\n\n# Create a sample summary dataframe\ndf = pl.DataFrame({\n    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n    'avg_mass_g': [3701, 3733, 5076]\n})\n\n# 1. Database Connection URI\ndb_path = \"penguins_pl.db\"\nuri = f\"sqlite:///{db_path}\"\n\n# 2. Write to Database\nprint(f\"--- Exporting to {db_path} ---\")\ndf.write_database(\n    table_name=\"species_summary\",\n    connection=uri,\n    if_table_exists=\"replace\"\n)\nprint(f\"\u2705 Successfully wrote {len(df)} rows to 'species_summary' table.\")\n\n# Clean up\nif os.path.exists(db_path):\n    os.remove(db_path)",
    "language": "python",
    "output": "--- Exporting to penguins_pl.db ---\n\u2705 Successfully wrote 3 rows to 'species_summary' table.\n"
  },
  "output_database_duckdb": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"duckdb\",\n#     \"pandas\",\n# ]\n# ///\nimport duckdb\nimport os\n\n# 1. Connect to a persistent database file\ndb_path = \"penguins_ddb.db\"\ncon = duckdb.connect(db_path)\n\n# 2. Create data and write to table\ncon.sql(\"\"\"\n    CREATE OR REPLACE TABLE species_summary AS \n    SELECT * FROM (VALUES \n        ('Adelie', 3701), \n        ('Chinstrap', 3733), \n        ('Gentoo', 5076)\n    ) t(species, avg_mass_g)\n\"\"\")\nprint(f\"\u2705 Successfully created and wrote to 'species_summary' table in {db_path}.\")\n\n# 3. Quick verification query\nresult = con.sql(\"SELECT * FROM species_summary\").df()\nprint(\"\\n--- Rows read back from DB ---\")\nprint(result)\n\n# Close connection before cleanup\ncon.close()\n\n# Clean up\nif os.path.exists(db_path):\n    os.remove(db_path)",
    "language": "python",
    "output": "\u2705 Successfully created and wrote to 'species_summary' table in penguins_ddb.db.\n\n--- Rows read back from DB ---\n     species  avg_mass_g\n0     Adelie        3701\n1  Chinstrap        3733\n2     Gentoo        5076\n"
  },
  "output_database_bigquery": {
    "code": "# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-cloud-bigquery\",\n#     \"pandas\",\n# ]\n# ///\nimport pandas as pd\nfrom unittest.mock import MagicMock\n\n# ---------------------------------------------------------\n# Mocking BigQuery Client for Portability\n# ---------------------------------------------------------\nclass MockBQClient:\n    def __init__(self):\n        print(\"--- BigQuery Client Initialized (Mock) ---\")\n    \n    def load_table_from_dataframe(self, dataframe, table_id, job_config=None):\n        print(f\"\u2705 Loading {len(dataframe)} rows into BigQuery table: {table_id}\")\n        mock_job = MagicMock()\n        mock_job.result = lambda: print(f\"\u2705 Load Job Completed!\")\n        return mock_job\n\nclient = MockBQClient()\n\n# ---------------------------------------------------------\n# BigQuery Database Load (to_sql equivalent)\n# ---------------------------------------------------------\ndf = pd.DataFrame({\n    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n    'avg_mass_g': [3701, 3733, 5076]\n})\n\ntable_id = \"my-project.dataset.species_summary\"\n\n# Load dataframe into BigQuery\n# In real usage, you'd provide a job_config to specify schema or write_disposition\njob = client.load_table_from_dataframe(df, table_id)\njob.result() # Wait for job to finish\n\nprint(\"\\n--- Summary of Load (Simulated) ---\")\nprint(f\"Destination: {table_id}\")\nprint(f\"Rows Loaded: {len(df)}\")",
    "language": "python",
    "output": "--- BigQuery Client Initialized (Mock) ---\n\u2705 Loading 3 rows into BigQuery table: my-project.dataset.species_summary\n\u2705 Load Job Completed!\n\n--- Summary of Load (Simulated) ---\nDestination: my-project.dataset.species_summary\nRows Loaded: 3\n"
  }
}
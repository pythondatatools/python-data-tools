{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <output_bigquery_polars>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"polars\",\n",
    "#     \"google-cloud-bigquery\",\n",
    "#     \"pyarrow\",\n",
    "#     \"pandas\",\n",
    "# ]\n",
    "# ///\n",
    "import polars as pl\n",
    "from unittest.mock import MagicMock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146978a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "BigQuery Loading Patterns for Polars\n",
    "---------------------------------------------------------\n",
    "Polars doesn't have a direct \"write_bigquery\" yet.\n",
    "Standard practice is to:\n",
    "1. Convert to Parquet/Arrow and use BQ Load Job\n",
    "2. Use BQ Storage Write API (via adbc or other connectors)\n",
    "3. Convert to Pandas as a fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\n",
    "    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n",
    "    'count': [152, 68, 124]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091cc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- BigQuery Loading via Arrow (Simulated) ---\")\n",
    "# Pattern: Write to a buffer and send to BQ\n",
    "import io\n",
    "buffer = io.BytesIO()\n",
    "df.write_parquet(buffer)\n",
    "print(f\"✅ Serialized {len(df)} rows to Parquet buffer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d136e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mocking the load from buffer\n",
    "print(f\"✅ BigQuery Load Job: Buffer -> table 'analytics.v1.penguin_counts'\")\n",
    "print(\"✅ Load Job Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef10e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPro Tip: For massive datasets, export Polars to Parquet in GCS first,\")\n",
    "print(\"then trigger a BigQuery 'Load' job from the GCS URI.\")\n",
    "# </output_bigquery_polars>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

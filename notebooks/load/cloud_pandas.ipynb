{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <load_cloud_pandas>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"pandas\",\n",
    "#     \"s3fs\",\n",
    "#     \"gcsfs\",\n",
    "#     \"pyarrow\",\n",
    "# ]\n",
    "# ///\n",
    "import pandas as pd\n",
    "import unittest.mock as mock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9cb8d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "AWS S3 (Simple Storage Service)\n",
    "---------------------------------------------------------\n",
    "Public dataset: NYC Taxi Data (Yellow Taxi, Jan 2023)\n",
    "Note: Real S3 access requires AWS credentials configured.\n",
    "We wrap this in a try/except block for demonstration resilience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af1c3f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Attempting to read from {S3_URI}...\")\n",
    "    # 'anon': True is required for public buckets without credentials\n",
    "    # Adding timeouts to fail fast if network is down\n",
    "    df_s3 = pd.read_parquet(S3_URI, storage_options={\n",
    "        \"anon\": True,\n",
    "        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n",
    "    })\n",
    "    print(\"Success natively reading S3!\")\n",
    "    print(df_s3.head())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    # Mock DataFrame\n",
    "    df_s3 = pd.DataFrame({\n",
    "        \"VendorID\": [1, 2, 1],\n",
    "        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n",
    "        \"passenger_count\": [1.0, 1.0, 2.0],\n",
    "        \"trip_distance\": [0.97, 1.10, 0.20],\n",
    "        \"total_amount\": [9.30, 14.30, 12.30]\n",
    "    })\n",
    "    print(df_s3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe0a81",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "Google Cloud Storage (GCS)\n",
    "---------------------------------------------------------\n",
    "Public dataset: BigQuery Public Data (US States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d395c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"\\nAttempting to read from {GCS_URI}...\")\n",
    "    df_gcs = pd.read_csv(GCS_URI)\n",
    "    print(\"Success natively reading GCS!\")\n",
    "    print(df_gcs.head())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    df_gcs = pd.DataFrame({\n",
    "        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n",
    "        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n",
    "    })\n",
    "    print(df_gcs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cd2be",
   "metadata": {},
   "source": [
    "</load_cloud_pandas>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

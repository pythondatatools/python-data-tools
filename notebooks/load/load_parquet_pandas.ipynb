{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e218bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# <load_parquet_pandas>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"pandas\",\n",
    "#     \"pyarrow\",\n",
    "# ]\n",
    "# ///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1341a8",
   "metadata": {},
   "source": [
    "### Parquet Loading with Pandas\n",
    "Columnar data handling with PyArrow engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "# Standard \"Wrangling Hero\" dataset: Palmer Penguins\n",
    "CSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "DATA_PATH = pathlib.Path(\"penguins.parquet\")\n",
    "\n",
    "# Self-healing: Download and convert if missing\n",
    "if not DATA_PATH.exists():\n",
    "    csv_temp = pathlib.Path(\"penguins.csv\")\n",
    "    if not csv_temp.exists():\n",
    "        urllib.request.urlretrieve(CSV_URL, csv_temp)\n",
    "    pd.read_csv(csv_temp).to_parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet\n",
    "# Pandas uses the pyarrow engine by default for Parquet\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Pandas loaded penguins Parquet with {len(df)} rows.\")\n",
    "print(df.head())\n",
    "# </load_parquet_pandas>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

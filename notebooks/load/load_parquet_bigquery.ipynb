{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685a577",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# <load_parquet_bigquery>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"google-cloud-bigquery\",\n",
    "#     \"pandas\",\n",
    "#     \"pyarrow\",\n",
    "# ]\n",
    "# ///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7efd67",
   "metadata": {},
   "source": [
    "### Parquet Loading with BigQuery\n",
    "Demonstrates loading Parquet data into BigQuery tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c742bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest.mock as mock\n",
    "from google.cloud import bigquery\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "# Standard \"Wrangling Hero\" dataset: Palmer Penguins\n",
    "CSV_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "DATA_PATH = pathlib.Path(\"penguins.parquet\")\n",
    "\n",
    "# Self-healing: Download and convert if missing (Local mock data)\n",
    "if not DATA_PATH.exists():\n",
    "    csv_temp = pathlib.Path(\"penguins.csv\")\n",
    "    if not csv_temp.exists():\n",
    "        urllib.request.urlretrieve(CSV_URL, csv_temp)\n",
    "    import pandas as pd # Needed for conversion\n",
    "    pd.read_csv(csv_temp).to_parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385994d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock the client\n",
    "client = mock.MagicMock(spec=bigquery.Client)\n",
    "\n",
    "# Load configuration\n",
    "table_id = \"project.dataset.penguins_parquet\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87200a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger load\n",
    "# Parquet is the recommended format for BigQuery due to schema persistence\n",
    "with open(DATA_PATH, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "print(f\"Mock BigQuery Parquet load triggered for {table_id}\")\n",
    "# </load_parquet_bigquery>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

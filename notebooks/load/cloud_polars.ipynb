{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596af65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <load_cloud_polars>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"polars\",\n",
    "#     \"s3fs\",\n",
    "#     \"gcsfs\",\n",
    "#     \"fsspec\",\n",
    "# ]\n",
    "# ///\n",
    "import polars as pl\n",
    "import unittest.mock as mock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab37fc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "AWS S3 (Simple Storage Service)\n",
    "---------------------------------------------------------\n",
    "Using NYC Taxi Data (Parquet) - Public Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ee2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a867104",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Attempting to scan from {S3_URI}...\")\n",
    "    # Polars scan_parquet is lazy and highly optimized for cloud\n",
    "    # storage_options={\"anon\": True} for public buckets\n",
    "    # Adding timeouts to fail fast\n",
    "    q = pl.scan_parquet(S3_URI, storage_options={\n",
    "        \"anon\": True,\n",
    "        \"client_kwargs\": {\"connect_timeout\": 5, \"read_timeout\": 5}\n",
    "    })\n",
    "    \n",
    "    # Collect first 5 rows\n",
    "    df_s3 = q.limit(5).collect()\n",
    "    print(\"Success natively reading S3!\")\n",
    "    print(df_s3)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    df_s3 = pl.DataFrame({\n",
    "        \"VendorID\": [1, 2, 1],\n",
    "        \"tpep_pickup_datetime\": [\"2023-01-01 00:32:10\", \"2023-01-01 00:55:08\", \"2023-01-01 00:25:04\"],\n",
    "        \"trip_distance\": [0.97, 1.10, 0.20],\n",
    "        \"total_amount\": [9.30, 14.30, 12.30]\n",
    "    })\n",
    "    print(df_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52601f7",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "Google Cloud Storage (GCS)\n",
    "---------------------------------------------------------\n",
    "using BigQuery public data (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_URI = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"\\nAttempting to read from {GCS_URI}...\")\n",
    "    df_gcs = pl.read_csv(GCS_URI)\n",
    "    print(\"Success natively reading GCS!\")\n",
    "    print(df_gcs.head())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    df_gcs = pl.DataFrame({\n",
    "        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n",
    "        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n",
    "    })\n",
    "    print(df_gcs)\n",
    "# </load_cloud_polars>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <load_cloud_duckdb>\n",
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"duckdb\",\n",
    "#     \"pandas\",\n",
    "# ]\n",
    "# ///\n",
    "import duckdb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6dc29",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "AWS S3 (Simple Storage Service)\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b09708",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_URI = \"s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Attempting to query {S3_URI}...\")\n",
    "    # DuckDB's httpfs extension handles S3/GCS\n",
    "    duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    \n",
    "    # Configure for anonymous access usually works for public buckets\n",
    "    # duckdb.sql(\"SET s3_region='us-east-1';\")\n",
    "    \n",
    "    # Query directly\n",
    "    df_s3 = duckdb.sql(f\"SELECT * FROM read_parquet('{S3_URI}') LIMIT 5\").df()\n",
    "    print(\"Success natively reading S3!\")\n",
    "    print(df_s3)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: S3 Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    # Mock output\n",
    "    df_s3 = pd.DataFrame({\n",
    "        \"VendorID\": [1, 2, 1],\n",
    "        \"trip_distance\": [0.97, 1.10, 0.20],\n",
    "        \"total_amount\": [9.30, 14.30, 12.30]\n",
    "    })\n",
    "    print(df_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Google Cloud Storage (GCS)\n",
    "# ---------------------------------------------------------\n",
    "# DuckDB reads GCS via S3 compatibility layer or HTTPS for public files.\n",
    "# For public files, HTTPS is robust.\n",
    "HTTPS_URI = \"https://storage.googleapis.com/cloud-samples-data/bigquery/us-states/us-states.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fccd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"\\nAttempting to read from {HTTPS_URI}...\")\n",
    "    df_gcs = duckdb.sql(f\"SELECT * FROM read_csv_auto('{HTTPS_URI}') LIMIT 5\").df()\n",
    "    print(\"Success natively reading GCS (via HTTPS)!\")\n",
    "    print(df_gcs)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: GCS Access failed ({e}). Mocking success for demo.\")\n",
    "    \n",
    "    df_gcs = pd.DataFrame({\n",
    "        \"name\": [\"Alabama\", \"Alaska\", \"Arizona\"],\n",
    "        \"post_abbr\": [\"AL\", \"AK\", \"AZ\"]\n",
    "    })\n",
    "    print(df_gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fc098",
   "metadata": {},
   "source": [
    "</load_cloud_duckdb>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
